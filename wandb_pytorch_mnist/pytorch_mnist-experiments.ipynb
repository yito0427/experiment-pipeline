{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments名を指定して、学習ジョブを発行\n",
    "\n",
    "Python 3(PyTorch 1.10 Python 3.8 CPU Optimized)で動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Training using PyTorch\n",
    "\n",
    "参考：wandb\n",
    "\n",
    "https://github.com/wandb/examples/blob/master/examples/pytorch/pytorch-mnist-sagemaker/pytorch_mnist.ipynb\n",
    "\n",
    "SageMaker sample\n",
    "\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/main/hyperparameter_tuning/pytorch_mnist/hpo_pytorch_mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets widgetsnbextension # studioの場合は必要。要カーネル再起動"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch / torchvision : 1.11.0 / 0.5.0 -> NG\n",
    "* torch / torchvision : 1.10.0 / 0.5.0 -> NG\n",
    "* torch / torchvision : 1.5.0 / 0.5.0 -> OK\n",
    "* torch / torchvision : 1.7.0 / 0.5.0 -> OK\n",
    "* torch / torchvision : 1.9.0 / 0.10.0 -> NG\n",
    "* torch / torchvision : 1.10.2 / 0.11.3 -> NG\n",
    "* torch / torchvision : 1.9.1 / 0.10.1 -> NG\n",
    "* torch / torchvision : 1.8.0 / 0.9.0 -> OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch==1.8.0 # 1.4.0で動作確認済み # 1.11.0 NG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision==0.9.0 # 0.5.0で動作確認済み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カーネル再起動してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker using PyTorch.\n",
    "\n",
    "MNISTは、手書きの数字を分類するために広く使われているデータセットである。このデータセットは、手書き数字の28x28ピクセルのグレースケール画像70,000枚から構成される。このデータセットは60,000枚の学習画像と10,000枚のテスト画像に分割される。10個のクラスがあります（10桁の数字にそれぞれ1つずつ）。このチュートリアルでは、PyTorchを使ってSageMaker上でMNISTモデルを学習・テストする方法を紹介します。\n",
    "\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U sagemaker==2.87.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-mnist-wandb-studio'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.77.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "MNIST.mirrors = [\"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/\"]\n",
    "\n",
    "MNIST(\n",
    "    'data',\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-ap-northeast-1-201228711527/sagemaker/DEMO-pytorch-mnist-wandb-studio\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-northeast-1-201228711527/sagemaker/DEMO-pytorch-mnist-wandb-studio'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "The `mnist.py` script provides all the code we need for training and hosting a SageMaker model (`model_fn` function to load a model).\n",
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "* `SM_NUM_GPUS`: The number of gpus available in the current container.\n",
    "* `SM_CURRENT_HOST`: The name of the current container on the container network.\n",
    "* `SM_HOSTS`: JSON encoded list containing all the hosts .\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "Because the SageMaker imports the training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model as we do in this example, so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For example, the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwandb\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#import sagemaker_containers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\n",
      "\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "\n",
      "\u001b[37m# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \u001b[36mself\u001b[39;49;00m.conv1 = nn.Conv2d(\u001b[34m1\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2 = nn.Conv2d(\u001b[34m10\u001b[39;49;00m, \u001b[34m20\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2_drop = nn.Dropout2d()\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m320\u001b[39;49;00m, \u001b[34m50\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m50\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
      "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv1(x), \u001b[34m2\u001b[39;49;00m))\n",
      "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv2_drop(\u001b[36mself\u001b[39;49;00m.conv2(x)), \u001b[34m2\u001b[39;49;00m))\n",
      "        x = x.view(-\u001b[34m1\u001b[39;49;00m, \u001b[34m320\u001b[39;49;00m)\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc1(x))\n",
      "        x = F.dropout(x, training=\u001b[36mself\u001b[39;49;00m.training)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc2(x)\n",
      "        \u001b[34mreturn\u001b[39;49;00m F.log_softmax(x, dim=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir, is_distributed, **kwargs):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    dataset = datasets.MNIST(\n",
      "        training_dir,\n",
      "        train=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        transform=transforms.Compose(\n",
      "            [transforms.ToTensor(), transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))]\n",
      "        ),\n",
      "    )\n",
      "    train_sampler = (\n",
      "        torch.utils.data.distributed.DistributedSampler(dataset) \u001b[34mif\u001b[39;49;00m is_distributed \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "    )\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\n",
      "        dataset,\n",
      "        batch_size=batch_size,\n",
      "        shuffle=train_sampler \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m,\n",
      "        sampler=train_sampler,\n",
      "        **kwargs\n",
      "    )\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(test_batch_size, training_dir, **kwargs):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet test data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\n",
      "        datasets.MNIST(\n",
      "            training_dir,\n",
      "            train=\u001b[34mFalse\u001b[39;49;00m,\n",
      "            transform=transforms.Compose(\n",
      "                [transforms.ToTensor(), transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))]\n",
      "            ),\n",
      "        ),\n",
      "        batch_size=test_batch_size,\n",
      "        shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        **kwargs\n",
      "    )\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_average_gradients\u001b[39;49;00m(model):\n",
      "    \u001b[37m# Gradient averaging.\u001b[39;49;00m\n",
      "    size = \u001b[36mfloat\u001b[39;49;00m(dist.get_world_size())\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
      "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
      "        param.grad.data /= size\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\n",
      "    is_distributed = \u001b[36mlen\u001b[39;49;00m(args.hosts) > \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.backend \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDistributed training - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(is_distributed))\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.num_gpus))\n",
      "    kwargs = {\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed:\n",
      "        \u001b[37m# Initialize the distributed environment.\u001b[39;49;00m\n",
      "        world_size = \u001b[36mlen\u001b[39;49;00m(args.hosts)\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(world_size)\n",
      "        host_rank = args.hosts.index(args.current_host)\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mRANK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(host_rank)\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
      "        logger.info(\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mInitialized the distributed environment: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m backend on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m nodes. \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "                args.backend, dist.get_world_size()\n",
      "            )\n",
      "            + \u001b[33m\"\u001b[39;49;00m\u001b[33mCurrent host rank is \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m. Number of gpus: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(dist.get_rank(), args.num_gpus)\n",
      "        )\n",
      "\n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\n",
      "    torch.manual_seed(args.seed)\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\n",
      "        torch.cuda.manual_seed(args.seed)\n",
      "\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\n",
      "    test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir, **kwargs)\n",
      "\n",
      "    logger.debug(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\n",
      "            \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "        )\n",
      "    )\n",
      "\n",
      "    logger.debug(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of test data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "            \u001b[36mlen\u001b[39;49;00m(test_loader.sampler),\n",
      "            \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "            \u001b[34m100.0\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(test_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "        )\n",
      "    )\n",
      "\n",
      "    model = Net().to(device)\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m use_cuda:\n",
      "        \u001b[37m# multi-machine multi-gpu case\u001b[39;49;00m\n",
      "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;49;00m\n",
      "        model = torch.nn.DataParallel(model)\n",
      "\n",
      "    wandb.watch(model)\n",
      "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        model.train()\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            optimizer.zero_grad()\n",
      "            output = model(data)\n",
      "            loss = F.nll_loss(output, target)\n",
      "            loss.backward()\n",
      "            \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m use_cuda:\n",
      "                \u001b[37m# average gradients manually for multi-machine cpu case only\u001b[39;49;00m\n",
      "                _average_gradients(model)\n",
      "            optimizer.step()\n",
      "            wandb.log({\u001b[33m\"\u001b[39;49;00m\u001b[33mtraining/loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: loss.item()})\n",
      "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\n",
      "                logger.info(\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mTrain Epoch: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m [\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: \u001b[39;49;00m\u001b[33m{:.6f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "                        epoch,\n",
      "                        batch_idx * \u001b[36mlen\u001b[39;49;00m(data),\n",
      "                        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\n",
      "                        \u001b[34m100.0\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader),\n",
      "                        loss.item(),\n",
      "                    )\n",
      "                )\n",
      "        test(model, test_loader, device)\n",
      "    save_model(model, args.model_dir)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device):\n",
      "    model.eval()\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m test_loader:\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            output = model(data)\n",
      "            test_loss += F.nll_loss(output, target, size_average=\u001b[34mFalse\u001b[39;49;00m).item()  \u001b[37m# sum up batch loss\u001b[39;49;00m\n",
      "            pred = output.max(\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[34mTrue\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m]  \u001b[37m# get the index of the max log-probability\u001b[39;49;00m\n",
      "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
      "\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "    wandb.log({\u001b[33m\"\u001b[39;49;00m\u001b[33mtesting/loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: test_loss})\n",
      "    logger.info(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mTest set: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m, Accuracy: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m{:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "            test_loss, correct, \u001b[36mlen\u001b[39;49;00m(test_loader.dataset), \u001b[34m100.0\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "        )\n",
      "    )\n",
      "    \u001b[37m# data and prediction visualization via W&B Tables\u001b[39;49;00m\n",
      "    original_data = datasets.MNIST(\n",
      "        args.data_dir,\n",
      "        train=\u001b[34mFalse\u001b[39;49;00m,\n",
      "    )\n",
      "    images, labels, preds = [], [], []\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m100\u001b[39;49;00m):\n",
      "        images += [original_data[i][\u001b[34m0\u001b[39;49;00m]]\n",
      "        labels += [original_data[i][\u001b[34m1\u001b[39;49;00m]]\n",
      "    processed_images = torch.stack([transforms.Compose(\n",
      "        [transforms.ToTensor(), transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))]\n",
      "    )(image) \u001b[34mfor\u001b[39;49;00m image \u001b[35min\u001b[39;49;00m images]).to(device)\n",
      "    output = model(processed_images).exp()\n",
      "    probs, preds = output.max(\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    probs, preds = probs.flatten(), preds.flatten()\n",
      "    table = []\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(images)):\n",
      "        table += [[wandb.Image(images[i]), labels[i], preds[i].item(), probs[i].item()]]\n",
      "    table = wandb.Table(data=table, columns=[\u001b[33m\"\u001b[39;49;00m\u001b[33mimage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mprediction\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mprobability\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    wandb.log({\u001b[33m\"\u001b[39;49;00m\u001b[33mmnist_visualization\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: table})\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = torch.nn.DataParallel(Net())\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[37m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;49;00m\n",
      "    torch.save(model.cpu().state_dict(), path)\n",
      "    wandb.save(path)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m1000\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m10\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m100\u001b[39;49;00m,\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    args = parser.parse_args()\n",
      "    wandb.init(project=\u001b[33m\"\u001b[39;49;00m\u001b[33msm-pytorch-mnist-studio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config=\u001b[36mvars\u001b[39;49;00m(args))\n",
      "    train(args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training in SageMaker\n",
    "\n",
    "The `PyTorch` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. In this case we are going to run our training job on 2 ```ml.c4.xlarge``` instances. But this example can be ran on one or multiple, cpu or gpu instances ([full list of available instances](https://aws.amazon.com/sagemaker/pricing/instance-types/)). The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the `mnist.py` script above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myito\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "settings = wandb.setup().settings\n",
    "current_api_key = wandb.wandb_lib.apikey.api_key(settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nestimator = PyTorch(entry_point=\\'mnist.py\\',\\n                    source_dir=\"src\",\\n                    role=role,\\n                    py_version=\\'py3\\',\\n                    framework_version=\\'1.8.0\\',\\n                    instance_count=1,\\n                    instance_type=\\'ml.c5.2xlarge\\',\\n                    hyperparameters={\\n                        \\'epochs\\': 1,\\n                        \\'backend\\': \\'gloo\\'\\n                    },\\n                   environment={\"WANDB_API_KEY\": current_api_key})\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "'''\n",
    "estimator = PyTorch(entry_point='mnist.py',\n",
    "                    source_dir=\"src\",\n",
    "                    role=role,\n",
    "                    py_version='py3',\n",
    "                    framework_version='1.8.0',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.c5.2xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 1,\n",
    "                        'backend': 'gloo'\n",
    "                    },\n",
    "                   environment={\"WANDB_API_KEY\": current_api_key})\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習ジョブのエスティメーターを定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point='mnist.py',\n",
    "                    source_dir=\"src\",\n",
    "                    role=role,\n",
    "                    py_version='py3',\n",
    "                    framework_version='1.8.0',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.c5.2xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 100,\n",
    "                        'backend': 'gloo'\n",
    "                    },\n",
    "                    metric_definitions=[\n",
    "                        {'Name': 'training/loss', 'Regex': 'Loss: (.*?)#015'},\n",
    "                        #{'Name': 'Validation Loss', 'Regex': 'val_loss: (.*?);'},\n",
    "                        {'Name': 'testing/loss', 'Regex': 'Average loss: (.*?),'},\n",
    "                        {'Name': 'Epoch', 'Regex': 'Epoch: (.*?) '},\n",
    "                        {'Name': 'epoch', 'Regex': 'Epoch: (.*?) '},\n",
    "                    ],\n",
    "                    environment={\"WANDB_API_KEY\": current_api_key})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この設定はStepFunctionsでパイプラインを作るとき、MLOpsエンジニア側の作業となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "import boto3\n",
    "sm = boto3.Session().client('sagemaker')\n",
    "from smexperiments.tracker import Tracker\n",
    "import time\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "# experimentの作成\n",
    "experiment = Experiment.create(experiment_name=\"mlops-experiment1002\", \n",
    "                               description=\"Sample Experiments for MLOps.\", \n",
    "                               sagemaker_boto_client=sm)\n",
    "\n",
    "# trackerの作成\n",
    "with Tracker.create(display_name=f\"tracker-{int(time.time())}\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_input(name=\"input-dataset-dir\", media_type=\"s3/uri\", value='s3://mlops/input/')\n",
    "    tracker.log_input(name=\"output-dataset-dir\", media_type=\"s3/uri\", value='s3://mlops/output/')\n",
    "\n",
    "# trialの作成\n",
    "trial_name = f\"training-job-{int(time.time())}\"\n",
    "experiments_trial = Trial.create(\n",
    "    trial_name=trial_name,\n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "\n",
    "# trial_componentの付与\n",
    "#sample_trial_component = tracker.trial_component\n",
    "#experiments_trial.add_trial_component(sample_trial_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_componentの付与\n",
    "estimator_trial_component = tracker.trial_component\n",
    "experiments_trial.add_trial_component(estimator_trial_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2022-05-31-05-11-19-197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-31 05:11:20 Starting - Starting the training job...\n",
      "2022-05-31 05:11:44 Starting - Preparing the instances for trainingProfilerReport-1653973879: InProgress\n",
      "......\n",
      "2022-05-31 05:12:49 Downloading - Downloading input data...\n",
      "2022-05-31 05:13:09 Training - Downloading the training image...\n",
      "2022-05-31 05:13:50 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-05-31 05:13:52,501 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-05-31 05:13:52,503 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-31 05:13:52,511 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-05-31 05:13:52,516 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-05-31 05:13:52,850 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting wandb\n",
      "  Downloading wandb-0.12.17-py2.py3-none-any.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\u001b[0m\n",
      "\u001b[34mCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (5.6.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.6/site-packages (from wandb->-r requirements.txt (line 1)) (3.15.6)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.0 in /opt/conda/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 1)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=1f527df4a7b98eb8a490d11431d955b0b205b4a1cb342e177aeb7e96835ee47d\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=3633c22d1514ed90c4e32deb68beafbdb188840a99c4f8b8e228d5ee4af08a3c\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/ea/90/e37d463fb3b03848bf715080595de62545266f53dd546b2497\u001b[0m\n",
      "\u001b[34mSuccessfully built promise pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, promise, pathtools, GitPython, docker-pycreds, wandb\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.18 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.17\u001b[0m\n",
      "\u001b[34m2022-05-31 05:13:57,864 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-31 05:13:57,877 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-31 05:13:57,889 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-31 05:13:57,900 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 100\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-05-31-05-11-19-197\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-1-201228711527/pytorch-training-2022-05-31-05-11-19-197/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":100}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-1-201228711527/pytorch-training-2022-05-31-05-11-19-197/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":100},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-05-31-05-11-19-197\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-1-201228711527/pytorch-training-2022-05-31-05-11-19-197/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 mnist.py --backend gloo --epochs 100\u001b[0m\n",
      "\u001b[34mDistributed training - False#015\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0#015\u001b[0m\n",
      "\u001b[34mGet train data loader#015\u001b[0m\n",
      "\u001b[34mGet test data loader#015\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data#015\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.329 algo-1:45 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.406 algo-1:45 INFO profiler_config_parser.py:102] User has disabled profiler.#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.407 algo-1:45 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.407 algo-1:45 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.408 algo-1:45 INFO hook.py:253] Saving to /opt/ml/output/tensors#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.408 algo-1:45 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.439 algo-1:45 INFO hook.py:584] name:module.conv1.weight count_params:250#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.440 algo-1:45 INFO hook.py:584] name:module.conv1.bias count_params:10#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.440 algo-1:45 INFO hook.py:584] name:module.conv2.weight count_params:5000#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.440 algo-1:45 INFO hook.py:584] name:module.conv2.bias count_params:20#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.440 algo-1:45 INFO hook.py:584] name:module.fc1.weight count_params:16000#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.440 algo-1:45 INFO hook.py:584] name:module.fc1.bias count_params:50#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.440 algo-1:45 INFO hook.py:584] name:module.fc2.weight count_params:500#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.440 algo-1:45 INFO hook.py:584] name:module.fc2.bias count_params:10#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.440 algo-1:45 INFO hook.py:586] Total Trainable Params: 21840#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.441 algo-1:45 INFO hook.py:413] Monitoring the collections: losses#015\u001b[0m\n",
      "\u001b[34m[2022-05-31 05:14:02.443 algo-1:45 INFO hook.py:476] Hook is writing from the hook with pid: 45#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)] Loss: 2.010908#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)] Loss: 1.009527#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)] Loss: 0.877578#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)] Loss: 0.805486#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)] Loss: 0.635695#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)] Loss: 0.505831#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)] Loss: 0.537033#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)] Loss: 0.532630#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)] Loss: 0.428416#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1919, Accuracy: 9424/10000 (94%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)] Loss: 0.516499#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)] Loss: 0.444650#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)] Loss: 0.510201#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)] Loss: 0.340110#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)] Loss: 0.361602#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)] Loss: 0.368281#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)] Loss: 0.245622#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)] Loss: 0.490514#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)] Loss: 0.419073#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1232, Accuracy: 9612/10000 (96%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)] Loss: 0.402352#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)] Loss: 0.422788#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)] Loss: 0.238820#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)] Loss: 0.512754#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)] Loss: 0.300689#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)] Loss: 0.205929#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)] Loss: 0.285849#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)] Loss: 0.183196#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)] Loss: 0.418420#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0971, Accuracy: 9699/10000 (97%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)] Loss: 0.487455#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)] Loss: 0.169946#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)] Loss: 0.482917#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)] Loss: 0.176369#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)] Loss: 0.243187#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)] Loss: 0.309636#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)] Loss: 0.345698#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)] Loss: 0.169519#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)] Loss: 0.419507#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0826, Accuracy: 9746/10000 (97%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)] Loss: 0.119444#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)] Loss: 0.246579#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)] Loss: 0.301410#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)] Loss: 0.134913#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)] Loss: 0.225903#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)] Loss: 0.265552#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)] Loss: 0.217658#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)] Loss: 0.126617#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)] Loss: 0.248121#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0714, Accuracy: 9776/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/60000 (11%)] Loss: 0.311248#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)] Loss: 0.193787#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/60000 (32%)] Loss: 0.279242#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)] Loss: 0.260202#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [32000/60000 (53%)] Loss: 0.146740#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)] Loss: 0.273261#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [44800/60000 (75%)] Loss: 0.126711#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)] Loss: 0.206522#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [57600/60000 (96%)] Loss: 0.108113#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0698, Accuracy: 9780/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/60000 (11%)] Loss: 0.208562#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)] Loss: 0.245645#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/60000 (32%)] Loss: 0.092395#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)] Loss: 0.204639#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32000/60000 (53%)] Loss: 0.148869#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)] Loss: 0.304543#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [44800/60000 (75%)] Loss: 0.212031#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)] Loss: 0.121765#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [57600/60000 (96%)] Loss: 0.094497#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0605, Accuracy: 9813/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/60000 (11%)] Loss: 0.123467#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)] Loss: 0.195638#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/60000 (32%)] Loss: 0.127997#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)] Loss: 0.129423#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [32000/60000 (53%)] Loss: 0.209255#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)] Loss: 0.108474#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [44800/60000 (75%)] Loss: 0.140764#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)] Loss: 0.167269#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [57600/60000 (96%)] Loss: 0.122999#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0549, Accuracy: 9833/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/60000 (11%)] Loss: 0.167448#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)] Loss: 0.166870#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/60000 (32%)] Loss: 0.239915#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)] Loss: 0.110166#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [32000/60000 (53%)] Loss: 0.172240#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)] Loss: 0.174107#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44800/60000 (75%)] Loss: 0.080643#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)] Loss: 0.271491#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [57600/60000 (96%)] Loss: 0.163187#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0534, Accuracy: 9834/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/60000 (11%)] Loss: 0.188100#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)] Loss: 0.323628#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/60000 (32%)] Loss: 0.265124#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)] Loss: 0.133954#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32000/60000 (53%)] Loss: 0.218809#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)] Loss: 0.190866#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44800/60000 (75%)] Loss: 0.155897#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)] Loss: 0.090382#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [57600/60000 (96%)] Loss: 0.121401#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0521, Accuracy: 9836/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [6400/60000 (11%)] Loss: 0.112356#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [12800/60000 (21%)] Loss: 0.106035#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [19200/60000 (32%)] Loss: 0.335787#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [25600/60000 (43%)] Loss: 0.101405#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [32000/60000 (53%)] Loss: 0.149416#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [38400/60000 (64%)] Loss: 0.106962#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [44800/60000 (75%)] Loss: 0.215564#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [51200/60000 (85%)] Loss: 0.225821#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 11 [57600/60000 (96%)] Loss: 0.111410#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0516, Accuracy: 9836/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [6400/60000 (11%)] Loss: 0.071913#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [12800/60000 (21%)] Loss: 0.092775#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [19200/60000 (32%)] Loss: 0.159508#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [25600/60000 (43%)] Loss: 0.146237#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [32000/60000 (53%)] Loss: 0.148256#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [38400/60000 (64%)] Loss: 0.161234#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [44800/60000 (75%)] Loss: 0.153370#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [51200/60000 (85%)] Loss: 0.156586#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 12 [57600/60000 (96%)] Loss: 0.034645#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0508, Accuracy: 9839/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [6400/60000 (11%)] Loss: 0.078203#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [12800/60000 (21%)] Loss: 0.173024#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [19200/60000 (32%)] Loss: 0.165137#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [25600/60000 (43%)] Loss: 0.243247#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [32000/60000 (53%)] Loss: 0.109689#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [38400/60000 (64%)] Loss: 0.164843#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [44800/60000 (75%)] Loss: 0.086418#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [51200/60000 (85%)] Loss: 0.414286#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 13 [57600/60000 (96%)] Loss: 0.216187#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0450, Accuracy: 9862/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [6400/60000 (11%)] Loss: 0.138607#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [12800/60000 (21%)] Loss: 0.119046#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [19200/60000 (32%)] Loss: 0.181357#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [25600/60000 (43%)] Loss: 0.141866#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [32000/60000 (53%)] Loss: 0.121717#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [38400/60000 (64%)] Loss: 0.128332#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [44800/60000 (75%)] Loss: 0.125014#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [51200/60000 (85%)] Loss: 0.127025#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 14 [57600/60000 (96%)] Loss: 0.050193#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0452, Accuracy: 9858/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [6400/60000 (11%)] Loss: 0.172882#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [12800/60000 (21%)] Loss: 0.143737#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [19200/60000 (32%)] Loss: 0.300406#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [25600/60000 (43%)] Loss: 0.077760#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [32000/60000 (53%)] Loss: 0.145571#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [38400/60000 (64%)] Loss: 0.023049#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [44800/60000 (75%)] Loss: 0.217329#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [51200/60000 (85%)] Loss: 0.117568#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 15 [57600/60000 (96%)] Loss: 0.245911#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0457, Accuracy: 9863/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [6400/60000 (11%)] Loss: 0.105229#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [12800/60000 (21%)] Loss: 0.056662#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [19200/60000 (32%)] Loss: 0.209752#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [25600/60000 (43%)] Loss: 0.150423#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [32000/60000 (53%)] Loss: 0.056701#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [38400/60000 (64%)] Loss: 0.086286#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [44800/60000 (75%)] Loss: 0.131011#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [51200/60000 (85%)] Loss: 0.250725#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 16 [57600/60000 (96%)] Loss: 0.131134#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0456, Accuracy: 9861/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [6400/60000 (11%)] Loss: 0.282895#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [12800/60000 (21%)] Loss: 0.138974#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [19200/60000 (32%)] Loss: 0.076126#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [25600/60000 (43%)] Loss: 0.152324#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [32000/60000 (53%)] Loss: 0.087691#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [38400/60000 (64%)] Loss: 0.166960#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [44800/60000 (75%)] Loss: 0.151961#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [51200/60000 (85%)] Loss: 0.084993#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 17 [57600/60000 (96%)] Loss: 0.099665#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0431, Accuracy: 9872/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [6400/60000 (11%)] Loss: 0.093555#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [12800/60000 (21%)] Loss: 0.124983#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [19200/60000 (32%)] Loss: 0.157423#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [25600/60000 (43%)] Loss: 0.234878#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [32000/60000 (53%)] Loss: 0.036059#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [38400/60000 (64%)] Loss: 0.100912#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [44800/60000 (75%)] Loss: 0.187551#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [51200/60000 (85%)] Loss: 0.083167#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 18 [57600/60000 (96%)] Loss: 0.078035#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0411, Accuracy: 9873/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [6400/60000 (11%)] Loss: 0.008723#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [12800/60000 (21%)] Loss: 0.129701#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [19200/60000 (32%)] Loss: 0.146162#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [25600/60000 (43%)] Loss: 0.068147#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [32000/60000 (53%)] Loss: 0.062070#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [38400/60000 (64%)] Loss: 0.258874#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [44800/60000 (75%)] Loss: 0.126667#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [51200/60000 (85%)] Loss: 0.041277#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 19 [57600/60000 (96%)] Loss: 0.131819#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0424, Accuracy: 9877/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [6400/60000 (11%)] Loss: 0.105172#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [12800/60000 (21%)] Loss: 0.216187#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [19200/60000 (32%)] Loss: 0.131778#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [25600/60000 (43%)] Loss: 0.230443#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [32000/60000 (53%)] Loss: 0.090155#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [38400/60000 (64%)] Loss: 0.130456#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [44800/60000 (75%)] Loss: 0.094917#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [51200/60000 (85%)] Loss: 0.044284#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 20 [57600/60000 (96%)] Loss: 0.206361#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0417, Accuracy: 9881/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [6400/60000 (11%)] Loss: 0.111291#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [12800/60000 (21%)] Loss: 0.110033#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [19200/60000 (32%)] Loss: 0.181939#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [25600/60000 (43%)] Loss: 0.050555#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [32000/60000 (53%)] Loss: 0.252834#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [38400/60000 (64%)] Loss: 0.166248#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [44800/60000 (75%)] Loss: 0.127529#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [51200/60000 (85%)] Loss: 0.033854#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 21 [57600/60000 (96%)] Loss: 0.064034#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0385, Accuracy: 9883/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [6400/60000 (11%)] Loss: 0.116788#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [12800/60000 (21%)] Loss: 0.112167#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [19200/60000 (32%)] Loss: 0.094450#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [25600/60000 (43%)] Loss: 0.243546#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [32000/60000 (53%)] Loss: 0.151123#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [38400/60000 (64%)] Loss: 0.254403#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [44800/60000 (75%)] Loss: 0.104938#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [51200/60000 (85%)] Loss: 0.101998#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 22 [57600/60000 (96%)] Loss: 0.042975#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0392, Accuracy: 9883/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [6400/60000 (11%)] Loss: 0.046421#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [12800/60000 (21%)] Loss: 0.142158#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [19200/60000 (32%)] Loss: 0.187064#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [25600/60000 (43%)] Loss: 0.053427#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [32000/60000 (53%)] Loss: 0.198804#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [38400/60000 (64%)] Loss: 0.110226#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [44800/60000 (75%)] Loss: 0.409861#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [51200/60000 (85%)] Loss: 0.132528#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 23 [57600/60000 (96%)] Loss: 0.263721#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0425, Accuracy: 9868/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [6400/60000 (11%)] Loss: 0.052292#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [12800/60000 (21%)] Loss: 0.044867#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [19200/60000 (32%)] Loss: 0.148333#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [25600/60000 (43%)] Loss: 0.088771#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [32000/60000 (53%)] Loss: 0.072909#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [38400/60000 (64%)] Loss: 0.263815#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [44800/60000 (75%)] Loss: 0.160303#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [51200/60000 (85%)] Loss: 0.110117#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 24 [57600/60000 (96%)] Loss: 0.102358#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0413, Accuracy: 9872/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [6400/60000 (11%)] Loss: 0.152091#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [12800/60000 (21%)] Loss: 0.301137#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [19200/60000 (32%)] Loss: 0.082108#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [25600/60000 (43%)] Loss: 0.139329#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [32000/60000 (53%)] Loss: 0.112130#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [38400/60000 (64%)] Loss: 0.081538#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [44800/60000 (75%)] Loss: 0.077450#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [51200/60000 (85%)] Loss: 0.174712#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 25 [57600/60000 (96%)] Loss: 0.189059#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0384, Accuracy: 9881/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [6400/60000 (11%)] Loss: 0.213804#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [12800/60000 (21%)] Loss: 0.070099#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [19200/60000 (32%)] Loss: 0.245567#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [25600/60000 (43%)] Loss: 0.174692#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [32000/60000 (53%)] Loss: 0.017653#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [38400/60000 (64%)] Loss: 0.044460#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [44800/60000 (75%)] Loss: 0.094395#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [51200/60000 (85%)] Loss: 0.120660#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 26 [57600/60000 (96%)] Loss: 0.083940#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0392, Accuracy: 9872/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [6400/60000 (11%)] Loss: 0.080526#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [12800/60000 (21%)] Loss: 0.071674#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [19200/60000 (32%)] Loss: 0.109580#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [25600/60000 (43%)] Loss: 0.340007#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [32000/60000 (53%)] Loss: 0.047298#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [38400/60000 (64%)] Loss: 0.029146#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [44800/60000 (75%)] Loss: 0.164960#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [51200/60000 (85%)] Loss: 0.191496#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 27 [57600/60000 (96%)] Loss: 0.061929#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0381, Accuracy: 9878/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [6400/60000 (11%)] Loss: 0.066284#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [12800/60000 (21%)] Loss: 0.070228#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [19200/60000 (32%)] Loss: 0.241526#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [25600/60000 (43%)] Loss: 0.288079#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [32000/60000 (53%)] Loss: 0.187028#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [38400/60000 (64%)] Loss: 0.066191#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [44800/60000 (75%)] Loss: 0.142586#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [51200/60000 (85%)] Loss: 0.213423#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 28 [57600/60000 (96%)] Loss: 0.050071#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0372, Accuracy: 9891/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [6400/60000 (11%)] Loss: 0.052445#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [12800/60000 (21%)] Loss: 0.173736#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [19200/60000 (32%)] Loss: 0.043981#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [25600/60000 (43%)] Loss: 0.199525#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [32000/60000 (53%)] Loss: 0.131625#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [38400/60000 (64%)] Loss: 0.238450#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [44800/60000 (75%)] Loss: 0.129466#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [51200/60000 (85%)] Loss: 0.092423#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 29 [57600/60000 (96%)] Loss: 0.070338#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0375, Accuracy: 9887/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [6400/60000 (11%)] Loss: 0.088298#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [12800/60000 (21%)] Loss: 0.135252#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [19200/60000 (32%)] Loss: 0.048693#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [25600/60000 (43%)] Loss: 0.174808#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [32000/60000 (53%)] Loss: 0.036013#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [38400/60000 (64%)] Loss: 0.117940#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [44800/60000 (75%)] Loss: 0.033974#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [51200/60000 (85%)] Loss: 0.059709#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 30 [57600/60000 (96%)] Loss: 0.113301#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0374, Accuracy: 9876/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [6400/60000 (11%)] Loss: 0.072627#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [12800/60000 (21%)] Loss: 0.102638#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [19200/60000 (32%)] Loss: 0.030296#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [25600/60000 (43%)] Loss: 0.236707#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [32000/60000 (53%)] Loss: 0.118573#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [38400/60000 (64%)] Loss: 0.117780#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [44800/60000 (75%)] Loss: 0.251511#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [51200/60000 (85%)] Loss: 0.149514#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 31 [57600/60000 (96%)] Loss: 0.043875#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0369, Accuracy: 9881/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [6400/60000 (11%)] Loss: 0.050862#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [12800/60000 (21%)] Loss: 0.082841#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [19200/60000 (32%)] Loss: 0.057284#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [25600/60000 (43%)] Loss: 0.119591#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [32000/60000 (53%)] Loss: 0.066195#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [38400/60000 (64%)] Loss: 0.043656#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [44800/60000 (75%)] Loss: 0.058437#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [51200/60000 (85%)] Loss: 0.026775#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 32 [57600/60000 (96%)] Loss: 0.078663#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0351, Accuracy: 9892/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [6400/60000 (11%)] Loss: 0.093559#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [12800/60000 (21%)] Loss: 0.190076#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [19200/60000 (32%)] Loss: 0.117135#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [25600/60000 (43%)] Loss: 0.275116#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [32000/60000 (53%)] Loss: 0.256858#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [38400/60000 (64%)] Loss: 0.081363#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [44800/60000 (75%)] Loss: 0.136691#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [51200/60000 (85%)] Loss: 0.062313#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 33 [57600/60000 (96%)] Loss: 0.075570#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0379, Accuracy: 9875/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [6400/60000 (11%)] Loss: 0.118693#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [12800/60000 (21%)] Loss: 0.066992#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [19200/60000 (32%)] Loss: 0.067611#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [25600/60000 (43%)] Loss: 0.018205#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [32000/60000 (53%)] Loss: 0.075057#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [38400/60000 (64%)] Loss: 0.164738#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [44800/60000 (75%)] Loss: 0.158538#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [51200/60000 (85%)] Loss: 0.021050#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 34 [57600/60000 (96%)] Loss: 0.147696#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0376, Accuracy: 9882/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [6400/60000 (11%)] Loss: 0.127798#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [12800/60000 (21%)] Loss: 0.144470#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [19200/60000 (32%)] Loss: 0.190596#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [25600/60000 (43%)] Loss: 0.040525#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [32000/60000 (53%)] Loss: 0.062947#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [38400/60000 (64%)] Loss: 0.198462#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [44800/60000 (75%)] Loss: 0.441351#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [51200/60000 (85%)] Loss: 0.112184#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 35 [57600/60000 (96%)] Loss: 0.068845#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0355, Accuracy: 9886/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [6400/60000 (11%)] Loss: 0.160976#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [12800/60000 (21%)] Loss: 0.142680#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [19200/60000 (32%)] Loss: 0.048189#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [25600/60000 (43%)] Loss: 0.167846#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [32000/60000 (53%)] Loss: 0.182378#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [38400/60000 (64%)] Loss: 0.009881#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [44800/60000 (75%)] Loss: 0.217505#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [51200/60000 (85%)] Loss: 0.334552#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 36 [57600/60000 (96%)] Loss: 0.273356#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0356, Accuracy: 9892/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [6400/60000 (11%)] Loss: 0.080823#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [12800/60000 (21%)] Loss: 0.154015#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [19200/60000 (32%)] Loss: 0.360232#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [25600/60000 (43%)] Loss: 0.216194#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [32000/60000 (53%)] Loss: 0.043174#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [38400/60000 (64%)] Loss: 0.132370#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [44800/60000 (75%)] Loss: 0.013444#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [51200/60000 (85%)] Loss: 0.113917#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 37 [57600/60000 (96%)] Loss: 0.074210#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0360, Accuracy: 9887/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [6400/60000 (11%)] Loss: 0.079570#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [12800/60000 (21%)] Loss: 0.065500#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [19200/60000 (32%)] Loss: 0.044250#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [25600/60000 (43%)] Loss: 0.182028#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [32000/60000 (53%)] Loss: 0.332636#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [38400/60000 (64%)] Loss: 0.093402#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [44800/60000 (75%)] Loss: 0.135959#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [51200/60000 (85%)] Loss: 0.097048#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 38 [57600/60000 (96%)] Loss: 0.092759#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0370, Accuracy: 9889/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [6400/60000 (11%)] Loss: 0.141053#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [12800/60000 (21%)] Loss: 0.056841#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [19200/60000 (32%)] Loss: 0.033757#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [25600/60000 (43%)] Loss: 0.115943#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [32000/60000 (53%)] Loss: 0.071690#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [38400/60000 (64%)] Loss: 0.116683#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [44800/60000 (75%)] Loss: 0.074361#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [51200/60000 (85%)] Loss: 0.181335#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 39 [57600/60000 (96%)] Loss: 0.063728#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0365, Accuracy: 9881/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [6400/60000 (11%)] Loss: 0.143994#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [12800/60000 (21%)] Loss: 0.046545#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [19200/60000 (32%)] Loss: 0.078724#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [25600/60000 (43%)] Loss: 0.061122#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [32000/60000 (53%)] Loss: 0.049731#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [38400/60000 (64%)] Loss: 0.099359#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [44800/60000 (75%)] Loss: 0.061095#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [51200/60000 (85%)] Loss: 0.042203#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 40 [57600/60000 (96%)] Loss: 0.236592#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0349, Accuracy: 9888/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [6400/60000 (11%)] Loss: 0.105668#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [12800/60000 (21%)] Loss: 0.161855#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [19200/60000 (32%)] Loss: 0.052601#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [25600/60000 (43%)] Loss: 0.082690#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [32000/60000 (53%)] Loss: 0.128884#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [38400/60000 (64%)] Loss: 0.075250#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [44800/60000 (75%)] Loss: 0.188635#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [51200/60000 (85%)] Loss: 0.299642#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 41 [57600/60000 (96%)] Loss: 0.103861#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0338, Accuracy: 9889/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [6400/60000 (11%)] Loss: 0.109927#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [12800/60000 (21%)] Loss: 0.065264#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [19200/60000 (32%)] Loss: 0.159770#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [25600/60000 (43%)] Loss: 0.011465#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [32000/60000 (53%)] Loss: 0.089540#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [38400/60000 (64%)] Loss: 0.230145#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [44800/60000 (75%)] Loss: 0.049102#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [51200/60000 (85%)] Loss: 0.095088#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 42 [57600/60000 (96%)] Loss: 0.140704#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0361, Accuracy: 9893/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [6400/60000 (11%)] Loss: 0.108548#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [12800/60000 (21%)] Loss: 0.143092#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [19200/60000 (32%)] Loss: 0.034835#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [25600/60000 (43%)] Loss: 0.055448#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [32000/60000 (53%)] Loss: 0.118246#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [38400/60000 (64%)] Loss: 0.135111#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [44800/60000 (75%)] Loss: 0.118639#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [51200/60000 (85%)] Loss: 0.109465#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 43 [57600/60000 (96%)] Loss: 0.080287#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0343, Accuracy: 9891/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [6400/60000 (11%)] Loss: 0.036705#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [12800/60000 (21%)] Loss: 0.068862#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [19200/60000 (32%)] Loss: 0.029734#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [25600/60000 (43%)] Loss: 0.100908#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [32000/60000 (53%)] Loss: 0.065856#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [38400/60000 (64%)] Loss: 0.065901#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [44800/60000 (75%)] Loss: 0.043819#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [51200/60000 (85%)] Loss: 0.208306#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 44 [57600/60000 (96%)] Loss: 0.020173#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0351, Accuracy: 9887/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [6400/60000 (11%)] Loss: 0.180203#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [12800/60000 (21%)] Loss: 0.063422#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [19200/60000 (32%)] Loss: 0.082477#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [25600/60000 (43%)] Loss: 0.095993#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [32000/60000 (53%)] Loss: 0.096017#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [38400/60000 (64%)] Loss: 0.025780#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [44800/60000 (75%)] Loss: 0.126184#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [51200/60000 (85%)] Loss: 0.025497#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 45 [57600/60000 (96%)] Loss: 0.066355#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0365, Accuracy: 9887/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [6400/60000 (11%)] Loss: 0.038022#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [12800/60000 (21%)] Loss: 0.253038#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [19200/60000 (32%)] Loss: 0.041427#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [25600/60000 (43%)] Loss: 0.200566#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [32000/60000 (53%)] Loss: 0.246226#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [38400/60000 (64%)] Loss: 0.242621#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [44800/60000 (75%)] Loss: 0.091123#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [51200/60000 (85%)] Loss: 0.139429#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 46 [57600/60000 (96%)] Loss: 0.048305#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0345, Accuracy: 9895/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [6400/60000 (11%)] Loss: 0.087813#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [12800/60000 (21%)] Loss: 0.175875#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [19200/60000 (32%)] Loss: 0.114143#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [25600/60000 (43%)] Loss: 0.117313#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [32000/60000 (53%)] Loss: 0.095179#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [38400/60000 (64%)] Loss: 0.271099#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [44800/60000 (75%)] Loss: 0.148610#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [51200/60000 (85%)] Loss: 0.048792#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 47 [57600/60000 (96%)] Loss: 0.106460#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0343, Accuracy: 9895/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [6400/60000 (11%)] Loss: 0.159438#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [12800/60000 (21%)] Loss: 0.102538#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [19200/60000 (32%)] Loss: 0.043739#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [25600/60000 (43%)] Loss: 0.056753#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [32000/60000 (53%)] Loss: 0.196211#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [38400/60000 (64%)] Loss: 0.155105#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [44800/60000 (75%)] Loss: 0.190288#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [51200/60000 (85%)] Loss: 0.298425#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 48 [57600/60000 (96%)] Loss: 0.120141#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0360, Accuracy: 9885/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [6400/60000 (11%)] Loss: 0.100056#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [12800/60000 (21%)] Loss: 0.038950#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [19200/60000 (32%)] Loss: 0.152304#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [25600/60000 (43%)] Loss: 0.254672#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [32000/60000 (53%)] Loss: 0.101048#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [38400/60000 (64%)] Loss: 0.068503#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [44800/60000 (75%)] Loss: 0.206642#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [51200/60000 (85%)] Loss: 0.050891#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 49 [57600/60000 (96%)] Loss: 0.174104#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0356, Accuracy: 9894/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [6400/60000 (11%)] Loss: 0.204063#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [12800/60000 (21%)] Loss: 0.173029#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [19200/60000 (32%)] Loss: 0.109832#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [25600/60000 (43%)] Loss: 0.227216#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [32000/60000 (53%)] Loss: 0.106297#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [38400/60000 (64%)] Loss: 0.005486#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [44800/60000 (75%)] Loss: 0.042832#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [51200/60000 (85%)] Loss: 0.074624#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 50 [57600/60000 (96%)] Loss: 0.038454#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0347, Accuracy: 9901/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [6400/60000 (11%)] Loss: 0.019694#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [12800/60000 (21%)] Loss: 0.018403#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [19200/60000 (32%)] Loss: 0.034463#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [25600/60000 (43%)] Loss: 0.020460#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [32000/60000 (53%)] Loss: 0.200257#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [38400/60000 (64%)] Loss: 0.060592#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [44800/60000 (75%)] Loss: 0.058124#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [51200/60000 (85%)] Loss: 0.077916#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 51 [57600/60000 (96%)] Loss: 0.059043#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0348, Accuracy: 9898/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [6400/60000 (11%)] Loss: 0.065269#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [12800/60000 (21%)] Loss: 0.379531#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [19200/60000 (32%)] Loss: 0.080676#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [25600/60000 (43%)] Loss: 0.135848#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [32000/60000 (53%)] Loss: 0.068347#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [38400/60000 (64%)] Loss: 0.005696#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [44800/60000 (75%)] Loss: 0.123619#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [51200/60000 (85%)] Loss: 0.127650#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 52 [57600/60000 (96%)] Loss: 0.116734#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0329, Accuracy: 9893/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [6400/60000 (11%)] Loss: 0.128041#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [12800/60000 (21%)] Loss: 0.085114#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [19200/60000 (32%)] Loss: 0.265462#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [25600/60000 (43%)] Loss: 0.055618#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [32000/60000 (53%)] Loss: 0.214720#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [38400/60000 (64%)] Loss: 0.094847#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [44800/60000 (75%)] Loss: 0.291006#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [51200/60000 (85%)] Loss: 0.129130#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 53 [57600/60000 (96%)] Loss: 0.037581#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0331, Accuracy: 9891/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [6400/60000 (11%)] Loss: 0.087481#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [12800/60000 (21%)] Loss: 0.179968#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [19200/60000 (32%)] Loss: 0.150827#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [25600/60000 (43%)] Loss: 0.111437#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [32000/60000 (53%)] Loss: 0.160384#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [38400/60000 (64%)] Loss: 0.070910#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [44800/60000 (75%)] Loss: 0.135381#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [51200/60000 (85%)] Loss: 0.142174#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 54 [57600/60000 (96%)] Loss: 0.019497#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0335, Accuracy: 9898/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [6400/60000 (11%)] Loss: 0.081622#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [12800/60000 (21%)] Loss: 0.045023#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [19200/60000 (32%)] Loss: 0.161775#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [25600/60000 (43%)] Loss: 0.102856#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [32000/60000 (53%)] Loss: 0.036828#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [38400/60000 (64%)] Loss: 0.344084#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [44800/60000 (75%)] Loss: 0.129132#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [51200/60000 (85%)] Loss: 0.204599#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 55 [57600/60000 (96%)] Loss: 0.190473#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0339, Accuracy: 9893/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [6400/60000 (11%)] Loss: 0.052145#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [12800/60000 (21%)] Loss: 0.134496#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [19200/60000 (32%)] Loss: 0.179634#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [25600/60000 (43%)] Loss: 0.034100#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [32000/60000 (53%)] Loss: 0.050627#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [38400/60000 (64%)] Loss: 0.077196#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [44800/60000 (75%)] Loss: 0.157859#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [51200/60000 (85%)] Loss: 0.087625#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 56 [57600/60000 (96%)] Loss: 0.299618#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0319, Accuracy: 9899/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [6400/60000 (11%)] Loss: 0.026706#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [12800/60000 (21%)] Loss: 0.090827#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [19200/60000 (32%)] Loss: 0.061407#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [25600/60000 (43%)] Loss: 0.023857#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [32000/60000 (53%)] Loss: 0.034828#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [38400/60000 (64%)] Loss: 0.147661#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [44800/60000 (75%)] Loss: 0.067487#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [51200/60000 (85%)] Loss: 0.040971#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 57 [57600/60000 (96%)] Loss: 0.050816#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0325, Accuracy: 9898/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [6400/60000 (11%)] Loss: 0.130971#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [12800/60000 (21%)] Loss: 0.106368#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [19200/60000 (32%)] Loss: 0.121631#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [25600/60000 (43%)] Loss: 0.229219#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [32000/60000 (53%)] Loss: 0.187055#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [38400/60000 (64%)] Loss: 0.064304#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [44800/60000 (75%)] Loss: 0.018331#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [51200/60000 (85%)] Loss: 0.010630#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 58 [57600/60000 (96%)] Loss: 0.169514#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0337, Accuracy: 9889/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [6400/60000 (11%)] Loss: 0.103137#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [12800/60000 (21%)] Loss: 0.058214#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [19200/60000 (32%)] Loss: 0.275803#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [25600/60000 (43%)] Loss: 0.119863#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [32000/60000 (53%)] Loss: 0.216173#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [38400/60000 (64%)] Loss: 0.065875#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [44800/60000 (75%)] Loss: 0.079435#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [51200/60000 (85%)] Loss: 0.037891#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 59 [57600/60000 (96%)] Loss: 0.117010#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0335, Accuracy: 9898/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [6400/60000 (11%)] Loss: 0.031274#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [12800/60000 (21%)] Loss: 0.067187#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [19200/60000 (32%)] Loss: 0.242228#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [25600/60000 (43%)] Loss: 0.179213#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [32000/60000 (53%)] Loss: 0.174482#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [38400/60000 (64%)] Loss: 0.070687#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [44800/60000 (75%)] Loss: 0.187256#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [51200/60000 (85%)] Loss: 0.112936#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 60 [57600/60000 (96%)] Loss: 0.240457#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0336, Accuracy: 9896/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [6400/60000 (11%)] Loss: 0.165502#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [12800/60000 (21%)] Loss: 0.050630#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [19200/60000 (32%)] Loss: 0.051875#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [25600/60000 (43%)] Loss: 0.076306#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [32000/60000 (53%)] Loss: 0.019781#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [38400/60000 (64%)] Loss: 0.231782#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [44800/60000 (75%)] Loss: 0.024237#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [51200/60000 (85%)] Loss: 0.107273#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 61 [57600/60000 (96%)] Loss: 0.159676#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0339, Accuracy: 9898/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [6400/60000 (11%)] Loss: 0.074403#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [12800/60000 (21%)] Loss: 0.058190#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [19200/60000 (32%)] Loss: 0.139564#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [25600/60000 (43%)] Loss: 0.155171#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [32000/60000 (53%)] Loss: 0.145582#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [38400/60000 (64%)] Loss: 0.092231#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [44800/60000 (75%)] Loss: 0.112156#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [51200/60000 (85%)] Loss: 0.200901#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 62 [57600/60000 (96%)] Loss: 0.143675#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0324, Accuracy: 9903/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [6400/60000 (11%)] Loss: 0.245128#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [12800/60000 (21%)] Loss: 0.120037#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [19200/60000 (32%)] Loss: 0.048563#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [25600/60000 (43%)] Loss: 0.097951#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [32000/60000 (53%)] Loss: 0.030139#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [38400/60000 (64%)] Loss: 0.074505#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [44800/60000 (75%)] Loss: 0.093680#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [51200/60000 (85%)] Loss: 0.154194#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 63 [57600/60000 (96%)] Loss: 0.049299#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0333, Accuracy: 9902/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [6400/60000 (11%)] Loss: 0.107199#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [12800/60000 (21%)] Loss: 0.248483#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [19200/60000 (32%)] Loss: 0.136995#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [25600/60000 (43%)] Loss: 0.029165#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [32000/60000 (53%)] Loss: 0.134294#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [38400/60000 (64%)] Loss: 0.068187#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [44800/60000 (75%)] Loss: 0.113829#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [51200/60000 (85%)] Loss: 0.074076#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 64 [57600/60000 (96%)] Loss: 0.065282#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0341, Accuracy: 9893/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [6400/60000 (11%)] Loss: 0.185330#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [12800/60000 (21%)] Loss: 0.077942#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [19200/60000 (32%)] Loss: 0.088431#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [25600/60000 (43%)] Loss: 0.048355#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [32000/60000 (53%)] Loss: 0.087714#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [38400/60000 (64%)] Loss: 0.095065#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [44800/60000 (75%)] Loss: 0.027056#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [51200/60000 (85%)] Loss: 0.057265#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 65 [57600/60000 (96%)] Loss: 0.054746#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0347, Accuracy: 9892/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [6400/60000 (11%)] Loss: 0.059838#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [12800/60000 (21%)] Loss: 0.060135#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [19200/60000 (32%)] Loss: 0.038968#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [25600/60000 (43%)] Loss: 0.029561#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [32000/60000 (53%)] Loss: 0.076519#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [38400/60000 (64%)] Loss: 0.062314#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [44800/60000 (75%)] Loss: 0.087165#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [51200/60000 (85%)] Loss: 0.077644#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 66 [57600/60000 (96%)] Loss: 0.012222#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0320, Accuracy: 9899/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [6400/60000 (11%)] Loss: 0.049246#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [12800/60000 (21%)] Loss: 0.043905#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [19200/60000 (32%)] Loss: 0.069530#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [25600/60000 (43%)] Loss: 0.123376#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [32000/60000 (53%)] Loss: 0.126443#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [38400/60000 (64%)] Loss: 0.044682#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [44800/60000 (75%)] Loss: 0.050416#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [51200/60000 (85%)] Loss: 0.127809#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 67 [57600/60000 (96%)] Loss: 0.076310#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0324, Accuracy: 9897/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [6400/60000 (11%)] Loss: 0.033351#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [12800/60000 (21%)] Loss: 0.203236#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [19200/60000 (32%)] Loss: 0.102300#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [25600/60000 (43%)] Loss: 0.058467#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [32000/60000 (53%)] Loss: 0.174342#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [38400/60000 (64%)] Loss: 0.109744#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [44800/60000 (75%)] Loss: 0.069767#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [51200/60000 (85%)] Loss: 0.063789#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 68 [57600/60000 (96%)] Loss: 0.036223#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0343, Accuracy: 9892/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [6400/60000 (11%)] Loss: 0.123904#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [12800/60000 (21%)] Loss: 0.039564#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [19200/60000 (32%)] Loss: 0.082209#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [25600/60000 (43%)] Loss: 0.010368#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [32000/60000 (53%)] Loss: 0.080549#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [38400/60000 (64%)] Loss: 0.093462#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [44800/60000 (75%)] Loss: 0.124006#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [51200/60000 (85%)] Loss: 0.054083#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 69 [57600/60000 (96%)] Loss: 0.195720#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0320, Accuracy: 9897/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [6400/60000 (11%)] Loss: 0.091919#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [12800/60000 (21%)] Loss: 0.111875#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [19200/60000 (32%)] Loss: 0.063098#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [25600/60000 (43%)] Loss: 0.054583#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [32000/60000 (53%)] Loss: 0.051599#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [38400/60000 (64%)] Loss: 0.148447#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [44800/60000 (75%)] Loss: 0.016186#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [51200/60000 (85%)] Loss: 0.061116#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 70 [57600/60000 (96%)] Loss: 0.155298#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0319, Accuracy: 9906/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [6400/60000 (11%)] Loss: 0.131618#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [12800/60000 (21%)] Loss: 0.033141#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [19200/60000 (32%)] Loss: 0.100116#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [25600/60000 (43%)] Loss: 0.045885#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [32000/60000 (53%)] Loss: 0.087338#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [38400/60000 (64%)] Loss: 0.169835#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [44800/60000 (75%)] Loss: 0.119572#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [51200/60000 (85%)] Loss: 0.044750#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 71 [57600/60000 (96%)] Loss: 0.240835#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0317, Accuracy: 9907/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [6400/60000 (11%)] Loss: 0.271560#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [12800/60000 (21%)] Loss: 0.028900#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [19200/60000 (32%)] Loss: 0.210168#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [25600/60000 (43%)] Loss: 0.109116#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [32000/60000 (53%)] Loss: 0.141454#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [38400/60000 (64%)] Loss: 0.168613#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [44800/60000 (75%)] Loss: 0.219056#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [51200/60000 (85%)] Loss: 0.060399#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 72 [57600/60000 (96%)] Loss: 0.043493#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0313, Accuracy: 9904/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [6400/60000 (11%)] Loss: 0.142548#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [12800/60000 (21%)] Loss: 0.281666#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [19200/60000 (32%)] Loss: 0.029255#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [25600/60000 (43%)] Loss: 0.055622#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [32000/60000 (53%)] Loss: 0.250654#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [38400/60000 (64%)] Loss: 0.102586#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [44800/60000 (75%)] Loss: 0.077563#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [51200/60000 (85%)] Loss: 0.046932#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 73 [57600/60000 (96%)] Loss: 0.151983#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0320, Accuracy: 9899/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [6400/60000 (11%)] Loss: 0.125363#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [12800/60000 (21%)] Loss: 0.039450#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [19200/60000 (32%)] Loss: 0.054856#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [25600/60000 (43%)] Loss: 0.099377#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [32000/60000 (53%)] Loss: 0.019091#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [38400/60000 (64%)] Loss: 0.047556#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [44800/60000 (75%)] Loss: 0.093003#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [51200/60000 (85%)] Loss: 0.100626#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 74 [57600/60000 (96%)] Loss: 0.162626#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0353, Accuracy: 9887/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [6400/60000 (11%)] Loss: 0.080359#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [12800/60000 (21%)] Loss: 0.063050#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [19200/60000 (32%)] Loss: 0.103407#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [25600/60000 (43%)] Loss: 0.014740#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [32000/60000 (53%)] Loss: 0.151761#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [38400/60000 (64%)] Loss: 0.087658#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [44800/60000 (75%)] Loss: 0.047844#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [51200/60000 (85%)] Loss: 0.030707#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 75 [57600/60000 (96%)] Loss: 0.046915#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0328, Accuracy: 9901/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [6400/60000 (11%)] Loss: 0.047835#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [12800/60000 (21%)] Loss: 0.144650#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [19200/60000 (32%)] Loss: 0.040655#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [25600/60000 (43%)] Loss: 0.035912#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [32000/60000 (53%)] Loss: 0.066303#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [38400/60000 (64%)] Loss: 0.081801#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [44800/60000 (75%)] Loss: 0.173158#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [51200/60000 (85%)] Loss: 0.134696#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 76 [57600/60000 (96%)] Loss: 0.161760#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0319, Accuracy: 9907/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [6400/60000 (11%)] Loss: 0.074871#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [12800/60000 (21%)] Loss: 0.234113#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [19200/60000 (32%)] Loss: 0.105561#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [25600/60000 (43%)] Loss: 0.066886#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [32000/60000 (53%)] Loss: 0.045934#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [38400/60000 (64%)] Loss: 0.033610#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [44800/60000 (75%)] Loss: 0.037894#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [51200/60000 (85%)] Loss: 0.065154#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 77 [57600/60000 (96%)] Loss: 0.032477#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0317, Accuracy: 9908/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [6400/60000 (11%)] Loss: 0.196764#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [12800/60000 (21%)] Loss: 0.038656#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [19200/60000 (32%)] Loss: 0.036117#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [25600/60000 (43%)] Loss: 0.034193#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [32000/60000 (53%)] Loss: 0.045344#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [38400/60000 (64%)] Loss: 0.084457#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [44800/60000 (75%)] Loss: 0.042022#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [51200/60000 (85%)] Loss: 0.306672#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 78 [57600/60000 (96%)] Loss: 0.030365#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0309, Accuracy: 9906/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [6400/60000 (11%)] Loss: 0.052691#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [12800/60000 (21%)] Loss: 0.129358#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [19200/60000 (32%)] Loss: 0.082864#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [25600/60000 (43%)] Loss: 0.041385#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [32000/60000 (53%)] Loss: 0.061863#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [38400/60000 (64%)] Loss: 0.050578#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [44800/60000 (75%)] Loss: 0.122971#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [51200/60000 (85%)] Loss: 0.079685#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 79 [57600/60000 (96%)] Loss: 0.066454#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0299, Accuracy: 9906/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [6400/60000 (11%)] Loss: 0.046151#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [12800/60000 (21%)] Loss: 0.055650#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [19200/60000 (32%)] Loss: 0.159340#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [25600/60000 (43%)] Loss: 0.143948#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [32000/60000 (53%)] Loss: 0.045080#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [38400/60000 (64%)] Loss: 0.019272#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [44800/60000 (75%)] Loss: 0.077514#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [51200/60000 (85%)] Loss: 0.042986#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 80 [57600/60000 (96%)] Loss: 0.091620#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0315, Accuracy: 9902/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [6400/60000 (11%)] Loss: 0.056343#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [12800/60000 (21%)] Loss: 0.037183#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [19200/60000 (32%)] Loss: 0.087564#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [25600/60000 (43%)] Loss: 0.077761#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [32000/60000 (53%)] Loss: 0.067879#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [38400/60000 (64%)] Loss: 0.108830#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [44800/60000 (75%)] Loss: 0.057337#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [51200/60000 (85%)] Loss: 0.079863#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 81 [57600/60000 (96%)] Loss: 0.021160#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0327, Accuracy: 9902/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [6400/60000 (11%)] Loss: 0.038890#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [12800/60000 (21%)] Loss: 0.092415#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [19200/60000 (32%)] Loss: 0.072560#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [25600/60000 (43%)] Loss: 0.030161#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [32000/60000 (53%)] Loss: 0.124448#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [38400/60000 (64%)] Loss: 0.069973#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [44800/60000 (75%)] Loss: 0.179310#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [51200/60000 (85%)] Loss: 0.062329#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 82 [57600/60000 (96%)] Loss: 0.077145#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0306, Accuracy: 9910/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [6400/60000 (11%)] Loss: 0.050432#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [12800/60000 (21%)] Loss: 0.148489#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [19200/60000 (32%)] Loss: 0.111924#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [25600/60000 (43%)] Loss: 0.188128#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [32000/60000 (53%)] Loss: 0.026280#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [38400/60000 (64%)] Loss: 0.012117#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [44800/60000 (75%)] Loss: 0.042773#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [51200/60000 (85%)] Loss: 0.130759#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 83 [57600/60000 (96%)] Loss: 0.120269#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0317, Accuracy: 9900/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [6400/60000 (11%)] Loss: 0.130953#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [12800/60000 (21%)] Loss: 0.068524#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [19200/60000 (32%)] Loss: 0.062708#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [25600/60000 (43%)] Loss: 0.124766#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [32000/60000 (53%)] Loss: 0.120848#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [38400/60000 (64%)] Loss: 0.185190#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [44800/60000 (75%)] Loss: 0.059152#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [51200/60000 (85%)] Loss: 0.041169#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 84 [57600/60000 (96%)] Loss: 0.197853#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0324, Accuracy: 9901/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [6400/60000 (11%)] Loss: 0.169204#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [12800/60000 (21%)] Loss: 0.025074#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [19200/60000 (32%)] Loss: 0.076920#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [25600/60000 (43%)] Loss: 0.022236#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [32000/60000 (53%)] Loss: 0.051466#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [38400/60000 (64%)] Loss: 0.030869#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [44800/60000 (75%)] Loss: 0.168445#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [51200/60000 (85%)] Loss: 0.050330#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 85 [57600/60000 (96%)] Loss: 0.040289#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0301, Accuracy: 9912/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [6400/60000 (11%)] Loss: 0.064346#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [12800/60000 (21%)] Loss: 0.161830#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [19200/60000 (32%)] Loss: 0.250034#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [25600/60000 (43%)] Loss: 0.196496#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [32000/60000 (53%)] Loss: 0.087708#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [38400/60000 (64%)] Loss: 0.087858#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [44800/60000 (75%)] Loss: 0.020622#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [51200/60000 (85%)] Loss: 0.048353#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 86 [57600/60000 (96%)] Loss: 0.111608#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0334, Accuracy: 9899/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [6400/60000 (11%)] Loss: 0.038829#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [12800/60000 (21%)] Loss: 0.092634#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [19200/60000 (32%)] Loss: 0.031339#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [25600/60000 (43%)] Loss: 0.045145#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [32000/60000 (53%)] Loss: 0.033340#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [38400/60000 (64%)] Loss: 0.015641#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [44800/60000 (75%)] Loss: 0.121507#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [51200/60000 (85%)] Loss: 0.049141#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 87 [57600/60000 (96%)] Loss: 0.008309#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0304, Accuracy: 9907/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [6400/60000 (11%)] Loss: 0.043641#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [12800/60000 (21%)] Loss: 0.259867#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [19200/60000 (32%)] Loss: 0.249357#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [25600/60000 (43%)] Loss: 0.017447#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [32000/60000 (53%)] Loss: 0.025209#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [38400/60000 (64%)] Loss: 0.126096#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [44800/60000 (75%)] Loss: 0.122665#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [51200/60000 (85%)] Loss: 0.116750#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 88 [57600/60000 (96%)] Loss: 0.143116#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0317, Accuracy: 9899/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [6400/60000 (11%)] Loss: 0.076101#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [12800/60000 (21%)] Loss: 0.093926#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [19200/60000 (32%)] Loss: 0.046323#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [25600/60000 (43%)] Loss: 0.160298#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [32000/60000 (53%)] Loss: 0.059707#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [38400/60000 (64%)] Loss: 0.108461#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [44800/60000 (75%)] Loss: 0.104760#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [51200/60000 (85%)] Loss: 0.122855#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 89 [57600/60000 (96%)] Loss: 0.184643#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0331, Accuracy: 9905/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [6400/60000 (11%)] Loss: 0.099659#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [12800/60000 (21%)] Loss: 0.048984#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [19200/60000 (32%)] Loss: 0.043429#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [25600/60000 (43%)] Loss: 0.049406#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [32000/60000 (53%)] Loss: 0.005740#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [38400/60000 (64%)] Loss: 0.074369#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [44800/60000 (75%)] Loss: 0.018109#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [51200/60000 (85%)] Loss: 0.079219#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 90 [57600/60000 (96%)] Loss: 0.062054#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0328, Accuracy: 9893/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [6400/60000 (11%)] Loss: 0.195896#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [12800/60000 (21%)] Loss: 0.114285#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [19200/60000 (32%)] Loss: 0.157614#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [25600/60000 (43%)] Loss: 0.068713#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [32000/60000 (53%)] Loss: 0.031988#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [38400/60000 (64%)] Loss: 0.276832#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [44800/60000 (75%)] Loss: 0.019260#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [51200/60000 (85%)] Loss: 0.115912#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 91 [57600/60000 (96%)] Loss: 0.084306#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0319, Accuracy: 9907/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [6400/60000 (11%)] Loss: 0.072547#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [12800/60000 (21%)] Loss: 0.045770#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [19200/60000 (32%)] Loss: 0.110325#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [25600/60000 (43%)] Loss: 0.117409#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [32000/60000 (53%)] Loss: 0.043965#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [38400/60000 (64%)] Loss: 0.061689#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [44800/60000 (75%)] Loss: 0.085295#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [51200/60000 (85%)] Loss: 0.047310#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 92 [57600/60000 (96%)] Loss: 0.076075#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0320, Accuracy: 9904/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [6400/60000 (11%)] Loss: 0.058955#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [12800/60000 (21%)] Loss: 0.002578#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [19200/60000 (32%)] Loss: 0.117825#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [25600/60000 (43%)] Loss: 0.124605#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [32000/60000 (53%)] Loss: 0.087118#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [38400/60000 (64%)] Loss: 0.122601#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [44800/60000 (75%)] Loss: 0.074069#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [51200/60000 (85%)] Loss: 0.075108#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 93 [57600/60000 (96%)] Loss: 0.072413#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0329, Accuracy: 9905/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [6400/60000 (11%)] Loss: 0.030512#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [12800/60000 (21%)] Loss: 0.068925#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [19200/60000 (32%)] Loss: 0.137267#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [25600/60000 (43%)] Loss: 0.042190#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [32000/60000 (53%)] Loss: 0.060325#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [38400/60000 (64%)] Loss: 0.041429#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [44800/60000 (75%)] Loss: 0.107717#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [51200/60000 (85%)] Loss: 0.102434#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 94 [57600/60000 (96%)] Loss: 0.019144#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0316, Accuracy: 9904/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [6400/60000 (11%)] Loss: 0.056783#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [12800/60000 (21%)] Loss: 0.080118#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [19200/60000 (32%)] Loss: 0.032920#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [25600/60000 (43%)] Loss: 0.026927#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [32000/60000 (53%)] Loss: 0.149326#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [38400/60000 (64%)] Loss: 0.049186#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [44800/60000 (75%)] Loss: 0.132033#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [51200/60000 (85%)] Loss: 0.038827#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 95 [57600/60000 (96%)] Loss: 0.062478#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0332, Accuracy: 9896/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [6400/60000 (11%)] Loss: 0.034881#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [12800/60000 (21%)] Loss: 0.092150#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [19200/60000 (32%)] Loss: 0.096903#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [25600/60000 (43%)] Loss: 0.266187#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [32000/60000 (53%)] Loss: 0.054809#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [38400/60000 (64%)] Loss: 0.061763#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [44800/60000 (75%)] Loss: 0.072388#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [51200/60000 (85%)] Loss: 0.081332#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 96 [57600/60000 (96%)] Loss: 0.053196#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0325, Accuracy: 9897/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [6400/60000 (11%)] Loss: 0.028507#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [12800/60000 (21%)] Loss: 0.059722#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [19200/60000 (32%)] Loss: 0.160479#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [25600/60000 (43%)] Loss: 0.147984#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [32000/60000 (53%)] Loss: 0.091091#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [38400/60000 (64%)] Loss: 0.105767#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [44800/60000 (75%)] Loss: 0.112072#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [51200/60000 (85%)] Loss: 0.022701#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 97 [57600/60000 (96%)] Loss: 0.265562#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0316, Accuracy: 9903/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [6400/60000 (11%)] Loss: 0.032463#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [12800/60000 (21%)] Loss: 0.038528#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [19200/60000 (32%)] Loss: 0.087709#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [25600/60000 (43%)] Loss: 0.073290#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [32000/60000 (53%)] Loss: 0.154298#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [38400/60000 (64%)] Loss: 0.096590#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [44800/60000 (75%)] Loss: 0.065880#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [51200/60000 (85%)] Loss: 0.103809#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 98 [57600/60000 (96%)] Loss: 0.058233#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0316, Accuracy: 9912/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [6400/60000 (11%)] Loss: 0.053864#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [12800/60000 (21%)] Loss: 0.079291#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [19200/60000 (32%)] Loss: 0.136426#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [25600/60000 (43%)] Loss: 0.033091#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [32000/60000 (53%)] Loss: 0.041876#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [38400/60000 (64%)] Loss: 0.055562#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [44800/60000 (75%)] Loss: 0.066481#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [51200/60000 (85%)] Loss: 0.058545#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 99 [57600/60000 (96%)] Loss: 0.173835#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0323, Accuracy: 9896/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [6400/60000 (11%)] Loss: 0.110165#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [12800/60000 (21%)] Loss: 0.197341#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [19200/60000 (32%)] Loss: 0.052914#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [25600/60000 (43%)] Loss: 0.028043#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [32000/60000 (53%)] Loss: 0.098189#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [38400/60000 (64%)] Loss: 0.136447#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [44800/60000 (75%)] Loss: 0.123920#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [51200/60000 (85%)] Loss: 0.095477#015\u001b[0m\n",
      "\u001b[34mTrain Epoch: 100 [57600/60000 (96%)] Loss: 0.034057#015\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0310, Accuracy: 9911/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mSaving the model.#015\u001b[0m\n",
      "\n",
      "2022-05-31 05:41:14 Uploading - Uploading generated training model\u001b[34mwandb: Currently logged in as: yito. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.12.17\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20220531_051400-pytorch-training-2022-05-31-05-11-19-197-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run pytorch-training-2022-05-31-05-11-19-197-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/yito/sm-pytorch-mnist-studio\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/yito/sm-pytorch-mnist-studio/runs/pytorch-training-2022-05-31-05-11-19-197-algo-1\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)] Loss: 2.010908#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)] Loss: 1.009527#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)] Loss: 0.877578#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)] Loss: 0.805486#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)] Loss: 0.635695#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)] Loss: 0.505831#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)] Loss: 0.537033#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)] Loss: 0.532630#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)] Loss: 0.428416#015\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.#015\n",
      "  warnings.warn(warning.format(ret))#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1919, Accuracy: 9424/10000 (94%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)] Loss: 0.516499#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)] Loss: 0.444650#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)] Loss: 0.510201#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)] Loss: 0.340110#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)] Loss: 0.361602#015\u001b[0m\n",
      "\u001b[34m2022-05-31 05:41:10,114 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)] Loss: 0.368281#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)] Loss: 0.245622#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)] Loss: 0.490514#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)] Loss: 0.419073#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.1232, Accuracy: 9612/10000 (96%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)] Loss: 0.402352#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)] Loss: 0.422788#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)] Loss: 0.238820#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)] Loss: 0.512754#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)] Loss: 0.300689#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)] Loss: 0.205929#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)] Loss: 0.285849#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)] Loss: 0.183196#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)] Loss: 0.418420#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0971, Accuracy: 9699/10000 (97%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)] Loss: 0.487455#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)] Loss: 0.169946#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)] Loss: 0.482917#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)] Loss: 0.176369#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)] Loss: 0.243187#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)] Loss: 0.309636#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)] Loss: 0.345698#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)] Loss: 0.169519#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)] Loss: 0.419507#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0826, Accuracy: 9746/10000 (97%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)] Loss: 0.119444#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)] Loss: 0.246579#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)] Loss: 0.301410#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)] Loss: 0.134913#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)] Loss: 0.225903#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)] Loss: 0.265552#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)] Loss: 0.217658#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)] Loss: 0.126617#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)] Loss: 0.248121#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0714, Accuracy: 9776/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/60000 (11%)] Loss: 0.311248#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/60000 (21%)] Loss: 0.193787#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/60000 (32%)] Loss: 0.279242#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/60000 (43%)] Loss: 0.260202#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [32000/60000 (53%)] Loss: 0.146740#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [38400/60000 (64%)] Loss: 0.273261#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [44800/60000 (75%)] Loss: 0.126711#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [51200/60000 (85%)] Loss: 0.206522#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [57600/60000 (96%)] Loss: 0.108113#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0698, Accuracy: 9780/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/60000 (11%)] Loss: 0.208562#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/60000 (21%)] Loss: 0.245645#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/60000 (32%)] Loss: 0.092395#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/60000 (43%)] Loss: 0.204639#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [32000/60000 (53%)] Loss: 0.148869#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [38400/60000 (64%)] Loss: 0.304543#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [44800/60000 (75%)] Loss: 0.212031#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [51200/60000 (85%)] Loss: 0.121765#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [57600/60000 (96%)] Loss: 0.094497#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0605, Accuracy: 9813/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/60000 (11%)] Loss: 0.123467#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/60000 (21%)] Loss: 0.195638#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/60000 (32%)] Loss: 0.127997#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/60000 (43%)] Loss: 0.129423#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [32000/60000 (53%)] Loss: 0.209255#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [38400/60000 (64%)] Loss: 0.108474#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [44800/60000 (75%)] Loss: 0.140764#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [51200/60000 (85%)] Loss: 0.167269#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [57600/60000 (96%)] Loss: 0.122999#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0549, Accuracy: 9833/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/60000 (11%)] Loss: 0.167448#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/60000 (21%)] Loss: 0.166870#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/60000 (32%)] Loss: 0.239915#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/60000 (43%)] Loss: 0.110166#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [32000/60000 (53%)] Loss: 0.172240#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [38400/60000 (64%)] Loss: 0.174107#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [44800/60000 (75%)] Loss: 0.080643#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [51200/60000 (85%)] Loss: 0.271491#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [57600/60000 (96%)] Loss: 0.163187#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0534, Accuracy: 9834/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/60000 (11%)] Loss: 0.188100#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/60000 (21%)] Loss: 0.323628#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/60000 (32%)] Loss: 0.265124#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/60000 (43%)] Loss: 0.133954#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [32000/60000 (53%)] Loss: 0.218809#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [38400/60000 (64%)] Loss: 0.190866#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [44800/60000 (75%)] Loss: 0.155897#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [51200/60000 (85%)] Loss: 0.090382#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [57600/60000 (96%)] Loss: 0.121401#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0521, Accuracy: 9836/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 11 [6400/60000 (11%)] Loss: 0.112356#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 11 [12800/60000 (21%)] Loss: 0.106035#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 11 [19200/60000 (32%)] Loss: 0.335787#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 11 [25600/60000 (43%)] Loss: 0.101405#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 11 [32000/60000 (53%)] Loss: 0.149416#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 11 [38400/60000 (64%)] Loss: 0.106962#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 11 [44800/60000 (75%)] Loss: 0.215564#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 11 [51200/60000 (85%)] Loss: 0.225821#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 11 [57600/60000 (96%)] Loss: 0.111410#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0516, Accuracy: 9836/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 12 [6400/60000 (11%)] Loss: 0.071913#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 12 [12800/60000 (21%)] Loss: 0.092775#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 12 [19200/60000 (32%)] Loss: 0.159508#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 12 [25600/60000 (43%)] Loss: 0.146237#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 12 [32000/60000 (53%)] Loss: 0.148256#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 12 [38400/60000 (64%)] Loss: 0.161234#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 12 [44800/60000 (75%)] Loss: 0.153370#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 12 [51200/60000 (85%)] Loss: 0.156586#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 12 [57600/60000 (96%)] Loss: 0.034645#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0508, Accuracy: 9839/10000 (98%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 13 [6400/60000 (11%)] Loss: 0.078203#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 13 [12800/60000 (21%)] Loss: 0.173024#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 13 [19200/60000 (32%)] Loss: 0.165137#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 13 [25600/60000 (43%)] Loss: 0.243247#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 13 [32000/60000 (53%)] Loss: 0.109689#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 13 [38400/60000 (64%)] Loss: 0.164843#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 13 [44800/60000 (75%)] Loss: 0.086418#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 13 [51200/60000 (85%)] Loss: 0.414286#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 13 [57600/60000 (96%)] Loss: 0.216187#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0450, Accuracy: 9862/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 14 [6400/60000 (11%)] Loss: 0.138607#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 14 [12800/60000 (21%)] Loss: 0.119046#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 14 [19200/60000 (32%)] Loss: 0.181357#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 14 [25600/60000 (43%)] Loss: 0.141866#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 14 [32000/60000 (53%)] Loss: 0.121717#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 14 [38400/60000 (64%)] Loss: 0.128332#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 14 [44800/60000 (75%)] Loss: 0.125014#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 14 [51200/60000 (85%)] Loss: 0.127025#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 14 [57600/60000 (96%)] Loss: 0.050193#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0452, Accuracy: 9858/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 15 [6400/60000 (11%)] Loss: 0.172882#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 15 [12800/60000 (21%)] Loss: 0.143737#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 15 [19200/60000 (32%)] Loss: 0.300406#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 15 [25600/60000 (43%)] Loss: 0.077760#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 15 [32000/60000 (53%)] Loss: 0.145571#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 15 [38400/60000 (64%)] Loss: 0.023049#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 15 [44800/60000 (75%)] Loss: 0.217329#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 15 [51200/60000 (85%)] Loss: 0.117568#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 15 [57600/60000 (96%)] Loss: 0.245911#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0457, Accuracy: 9863/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 16 [6400/60000 (11%)] Loss: 0.105229#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 16 [12800/60000 (21%)] Loss: 0.056662#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 16 [19200/60000 (32%)] Loss: 0.209752#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 16 [25600/60000 (43%)] Loss: 0.150423#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 16 [32000/60000 (53%)] Loss: 0.056701#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 16 [38400/60000 (64%)] Loss: 0.086286#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 16 [44800/60000 (75%)] Loss: 0.131011#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 16 [51200/60000 (85%)] Loss: 0.250725#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 16 [57600/60000 (96%)] Loss: 0.131134#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0456, Accuracy: 9861/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 17 [6400/60000 (11%)] Loss: 0.282895#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 17 [12800/60000 (21%)] Loss: 0.138974#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 17 [19200/60000 (32%)] Loss: 0.076126#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 17 [25600/60000 (43%)] Loss: 0.152324#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 17 [32000/60000 (53%)] Loss: 0.087691#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 17 [38400/60000 (64%)] Loss: 0.166960#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 17 [44800/60000 (75%)] Loss: 0.151961#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 17 [51200/60000 (85%)] Loss: 0.084993#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 17 [57600/60000 (96%)] Loss: 0.099665#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0431, Accuracy: 9872/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 18 [6400/60000 (11%)] Loss: 0.093555#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 18 [12800/60000 (21%)] Loss: 0.124983#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 18 [19200/60000 (32%)] Loss: 0.157423#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 18 [25600/60000 (43%)] Loss: 0.234878#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 18 [32000/60000 (53%)] Loss: 0.036059#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 18 [38400/60000 (64%)] Loss: 0.100912#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 18 [44800/60000 (75%)] Loss: 0.187551#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 18 [51200/60000 (85%)] Loss: 0.083167#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 18 [57600/60000 (96%)] Loss: 0.078035#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0411, Accuracy: 9873/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 19 [6400/60000 (11%)] Loss: 0.008723#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 19 [12800/60000 (21%)] Loss: 0.129701#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 19 [19200/60000 (32%)] Loss: 0.146162#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 19 [25600/60000 (43%)] Loss: 0.068147#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 19 [32000/60000 (53%)] Loss: 0.062070#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 19 [38400/60000 (64%)] Loss: 0.258874#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 19 [44800/60000 (75%)] Loss: 0.126667#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 19 [51200/60000 (85%)] Loss: 0.041277#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 19 [57600/60000 (96%)] Loss: 0.131819#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0424, Accuracy: 9877/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 20 [6400/60000 (11%)] Loss: 0.105172#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 20 [12800/60000 (21%)] Loss: 0.216187#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 20 [19200/60000 (32%)] Loss: 0.131778#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 20 [25600/60000 (43%)] Loss: 0.230443#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 20 [32000/60000 (53%)] Loss: 0.090155#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 20 [38400/60000 (64%)] Loss: 0.130456#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 20 [44800/60000 (75%)] Loss: 0.094917#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 20 [51200/60000 (85%)] Loss: 0.044284#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 20 [57600/60000 (96%)] Loss: 0.206361#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0417, Accuracy: 9881/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 21 [6400/60000 (11%)] Loss: 0.111291#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 21 [12800/60000 (21%)] Loss: 0.110033#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 21 [19200/60000 (32%)] Loss: 0.181939#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 21 [25600/60000 (43%)] Loss: 0.050555#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 21 [32000/60000 (53%)] Loss: 0.252834#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 21 [38400/60000 (64%)] Loss: 0.166248#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 21 [44800/60000 (75%)] Loss: 0.127529#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 21 [51200/60000 (85%)] Loss: 0.033854#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 21 [57600/60000 (96%)] Loss: 0.064034#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0385, Accuracy: 9883/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 22 [6400/60000 (11%)] Loss: 0.116788#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 22 [12800/60000 (21%)] Loss: 0.112167#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 22 [19200/60000 (32%)] Loss: 0.094450#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 22 [25600/60000 (43%)] Loss: 0.243546#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 22 [32000/60000 (53%)] Loss: 0.151123#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 22 [38400/60000 (64%)] Loss: 0.254403#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 22 [44800/60000 (75%)] Loss: 0.104938#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 22 [51200/60000 (85%)] Loss: 0.101998#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 22 [57600/60000 (96%)] Loss: 0.042975#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0392, Accuracy: 9883/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 23 [6400/60000 (11%)] Loss: 0.046421#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 23 [12800/60000 (21%)] Loss: 0.142158#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 23 [19200/60000 (32%)] Loss: 0.187064#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 23 [25600/60000 (43%)] Loss: 0.053427#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 23 [32000/60000 (53%)] Loss: 0.198804#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 23 [38400/60000 (64%)] Loss: 0.110226#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 23 [44800/60000 (75%)] Loss: 0.409861#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 23 [51200/60000 (85%)] Loss: 0.132528#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 23 [57600/60000 (96%)] Loss: 0.263721#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0425, Accuracy: 9868/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 24 [6400/60000 (11%)] Loss: 0.052292#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 24 [12800/60000 (21%)] Loss: 0.044867#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 24 [19200/60000 (32%)] Loss: 0.148333#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 24 [25600/60000 (43%)] Loss: 0.088771#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 24 [32000/60000 (53%)] Loss: 0.072909#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 24 [38400/60000 (64%)] Loss: 0.263815#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 24 [44800/60000 (75%)] Loss: 0.160303#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 24 [51200/60000 (85%)] Loss: 0.110117#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 24 [57600/60000 (96%)] Loss: 0.102358#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0413, Accuracy: 9872/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 25 [6400/60000 (11%)] Loss: 0.152091#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 25 [12800/60000 (21%)] Loss: 0.301137#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 25 [19200/60000 (32%)] Loss: 0.082108#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 25 [25600/60000 (43%)] Loss: 0.139329#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 25 [32000/60000 (53%)] Loss: 0.112130#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 25 [38400/60000 (64%)] Loss: 0.081538#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 25 [44800/60000 (75%)] Loss: 0.077450#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 25 [51200/60000 (85%)] Loss: 0.174712#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 25 [57600/60000 (96%)] Loss: 0.189059#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0384, Accuracy: 9881/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 26 [6400/60000 (11%)] Loss: 0.213804#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 26 [12800/60000 (21%)] Loss: 0.070099#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 26 [19200/60000 (32%)] Loss: 0.245567#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 26 [25600/60000 (43%)] Loss: 0.174692#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 26 [32000/60000 (53%)] Loss: 0.017653#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 26 [38400/60000 (64%)] Loss: 0.044460#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 26 [44800/60000 (75%)] Loss: 0.094395#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 26 [51200/60000 (85%)] Loss: 0.120660#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 26 [57600/60000 (96%)] Loss: 0.083940#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0392, Accuracy: 9872/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 27 [6400/60000 (11%)] Loss: 0.080526#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 27 [12800/60000 (21%)] Loss: 0.071674#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 27 [19200/60000 (32%)] Loss: 0.109580#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 27 [25600/60000 (43%)] Loss: 0.340007#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 27 [32000/60000 (53%)] Loss: 0.047298#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 27 [38400/60000 (64%)] Loss: 0.029146#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 27 [44800/60000 (75%)] Loss: 0.164960#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 27 [51200/60000 (85%)] Loss: 0.191496#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 27 [57600/60000 (96%)] Loss: 0.061929#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0381, Accuracy: 9878/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 28 [6400/60000 (11%)] Loss: 0.066284#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 28 [12800/60000 (21%)] Loss: 0.070228#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 28 [19200/60000 (32%)] Loss: 0.241526#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 28 [25600/60000 (43%)] Loss: 0.288079#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 28 [32000/60000 (53%)] Loss: 0.187028#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 28 [38400/60000 (64%)] Loss: 0.066191#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 28 [44800/60000 (75%)] Loss: 0.142586#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 28 [51200/60000 (85%)] Loss: 0.213423#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 28 [57600/60000 (96%)] Loss: 0.050071#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0372, Accuracy: 9891/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 29 [6400/60000 (11%)] Loss: 0.052445#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 29 [12800/60000 (21%)] Loss: 0.173736#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 29 [19200/60000 (32%)] Loss: 0.043981#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 29 [25600/60000 (43%)] Loss: 0.199525#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 29 [32000/60000 (53%)] Loss: 0.131625#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 29 [38400/60000 (64%)] Loss: 0.238450#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 29 [44800/60000 (75%)] Loss: 0.129466#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 29 [51200/60000 (85%)] Loss: 0.092423#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 29 [57600/60000 (96%)] Loss: 0.070338#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0375, Accuracy: 9887/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 30 [6400/60000 (11%)] Loss: 0.088298#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 30 [12800/60000 (21%)] Loss: 0.135252#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 30 [19200/60000 (32%)] Loss: 0.048693#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 30 [25600/60000 (43%)] Loss: 0.174808#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 30 [32000/60000 (53%)] Loss: 0.036013#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 30 [38400/60000 (64%)] Loss: 0.117940#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 30 [44800/60000 (75%)] Loss: 0.033974#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 30 [51200/60000 (85%)] Loss: 0.059709#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 30 [57600/60000 (96%)] Loss: 0.113301#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0374, Accuracy: 9876/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 31 [6400/60000 (11%)] Loss: 0.072627#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 31 [12800/60000 (21%)] Loss: 0.102638#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 31 [19200/60000 (32%)] Loss: 0.030296#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 31 [25600/60000 (43%)] Loss: 0.236707#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 31 [32000/60000 (53%)] Loss: 0.118573#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 31 [38400/60000 (64%)] Loss: 0.117780#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 31 [44800/60000 (75%)] Loss: 0.251511#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 31 [51200/60000 (85%)] Loss: 0.149514#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 31 [57600/60000 (96%)] Loss: 0.043875#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0369, Accuracy: 9881/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 32 [6400/60000 (11%)] Loss: 0.050862#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 32 [12800/60000 (21%)] Loss: 0.082841#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 32 [19200/60000 (32%)] Loss: 0.057284#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 32 [25600/60000 (43%)] Loss: 0.119591#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 32 [32000/60000 (53%)] Loss: 0.066195#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 32 [38400/60000 (64%)] Loss: 0.043656#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 32 [44800/60000 (75%)] Loss: 0.058437#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 32 [51200/60000 (85%)] Loss: 0.026775#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 32 [57600/60000 (96%)] Loss: 0.078663#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0351, Accuracy: 9892/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 33 [6400/60000 (11%)] Loss: 0.093559#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 33 [12800/60000 (21%)] Loss: 0.190076#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 33 [19200/60000 (32%)] Loss: 0.117135#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 33 [25600/60000 (43%)] Loss: 0.275116#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 33 [32000/60000 (53%)] Loss: 0.256858#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 33 [38400/60000 (64%)] Loss: 0.081363#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 33 [44800/60000 (75%)] Loss: 0.136691#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 33 [51200/60000 (85%)] Loss: 0.062313#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 33 [57600/60000 (96%)] Loss: 0.075570#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0379, Accuracy: 9875/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 34 [6400/60000 (11%)] Loss: 0.118693#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 34 [12800/60000 (21%)] Loss: 0.066992#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 34 [19200/60000 (32%)] Loss: 0.067611#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 34 [25600/60000 (43%)] Loss: 0.018205#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 34 [32000/60000 (53%)] Loss: 0.075057#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 34 [38400/60000 (64%)] Loss: 0.164738#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 34 [44800/60000 (75%)] Loss: 0.158538#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 34 [51200/60000 (85%)] Loss: 0.021050#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 34 [57600/60000 (96%)] Loss: 0.147696#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0376, Accuracy: 9882/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 35 [6400/60000 (11%)] Loss: 0.127798#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 35 [12800/60000 (21%)] Loss: 0.144470#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 35 [19200/60000 (32%)] Loss: 0.190596#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 35 [25600/60000 (43%)] Loss: 0.040525#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 35 [32000/60000 (53%)] Loss: 0.062947#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 35 [38400/60000 (64%)] Loss: 0.198462#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 35 [44800/60000 (75%)] Loss: 0.441351#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 35 [51200/60000 (85%)] Loss: 0.112184#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 35 [57600/60000 (96%)] Loss: 0.068845#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0355, Accuracy: 9886/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 36 [6400/60000 (11%)] Loss: 0.160976#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 36 [12800/60000 (21%)] Loss: 0.142680#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 36 [19200/60000 (32%)] Loss: 0.048189#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 36 [25600/60000 (43%)] Loss: 0.167846#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 36 [32000/60000 (53%)] Loss: 0.182378#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 36 [38400/60000 (64%)] Loss: 0.009881#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 36 [44800/60000 (75%)] Loss: 0.217505#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 36 [51200/60000 (85%)] Loss: 0.334552#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 36 [57600/60000 (96%)] Loss: 0.273356#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0356, Accuracy: 9892/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 37 [6400/60000 (11%)] Loss: 0.080823#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 37 [12800/60000 (21%)] Loss: 0.154015#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 37 [19200/60000 (32%)] Loss: 0.360232#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 37 [25600/60000 (43%)] Loss: 0.216194#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 37 [32000/60000 (53%)] Loss: 0.043174#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 37 [38400/60000 (64%)] Loss: 0.132370#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 37 [44800/60000 (75%)] Loss: 0.013444#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 37 [51200/60000 (85%)] Loss: 0.113917#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 37 [57600/60000 (96%)] Loss: 0.074210#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0360, Accuracy: 9887/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 38 [6400/60000 (11%)] Loss: 0.079570#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 38 [12800/60000 (21%)] Loss: 0.065500#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 38 [19200/60000 (32%)] Loss: 0.044250#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 38 [25600/60000 (43%)] Loss: 0.182028#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 38 [32000/60000 (53%)] Loss: 0.332636#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 38 [38400/60000 (64%)] Loss: 0.093402#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 38 [44800/60000 (75%)] Loss: 0.135959#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 38 [51200/60000 (85%)] Loss: 0.097048#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 38 [57600/60000 (96%)] Loss: 0.092759#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0370, Accuracy: 9889/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 39 [6400/60000 (11%)] Loss: 0.141053#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 39 [12800/60000 (21%)] Loss: 0.056841#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 39 [19200/60000 (32%)] Loss: 0.033757#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 39 [25600/60000 (43%)] Loss: 0.115943#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 39 [32000/60000 (53%)] Loss: 0.071690#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 39 [38400/60000 (64%)] Loss: 0.116683#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 39 [44800/60000 (75%)] Loss: 0.074361#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 39 [51200/60000 (85%)] Loss: 0.181335#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 39 [57600/60000 (96%)] Loss: 0.063728#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0365, Accuracy: 9881/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 40 [6400/60000 (11%)] Loss: 0.143994#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 40 [12800/60000 (21%)] Loss: 0.046545#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 40 [19200/60000 (32%)] Loss: 0.078724#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 40 [25600/60000 (43%)] Loss: 0.061122#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 40 [32000/60000 (53%)] Loss: 0.049731#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 40 [38400/60000 (64%)] Loss: 0.099359#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 40 [44800/60000 (75%)] Loss: 0.061095#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 40 [51200/60000 (85%)] Loss: 0.042203#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 40 [57600/60000 (96%)] Loss: 0.236592#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0349, Accuracy: 9888/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 41 [6400/60000 (11%)] Loss: 0.105668#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 41 [12800/60000 (21%)] Loss: 0.161855#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 41 [19200/60000 (32%)] Loss: 0.052601#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 41 [25600/60000 (43%)] Loss: 0.082690#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 41 [32000/60000 (53%)] Loss: 0.128884#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 41 [38400/60000 (64%)] Loss: 0.075250#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 41 [44800/60000 (75%)] Loss: 0.188635#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 41 [51200/60000 (85%)] Loss: 0.299642#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 41 [57600/60000 (96%)] Loss: 0.103861#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0338, Accuracy: 9889/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 42 [6400/60000 (11%)] Loss: 0.109927#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 42 [12800/60000 (21%)] Loss: 0.065264#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 42 [19200/60000 (32%)] Loss: 0.159770#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 42 [25600/60000 (43%)] Loss: 0.011465#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 42 [32000/60000 (53%)] Loss: 0.089540#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 42 [38400/60000 (64%)] Loss: 0.230145#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 42 [44800/60000 (75%)] Loss: 0.049102#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 42 [51200/60000 (85%)] Loss: 0.095088#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 42 [57600/60000 (96%)] Loss: 0.140704#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0361, Accuracy: 9893/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 43 [6400/60000 (11%)] Loss: 0.108548#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 43 [12800/60000 (21%)] Loss: 0.143092#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 43 [19200/60000 (32%)] Loss: 0.034835#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 43 [25600/60000 (43%)] Loss: 0.055448#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 43 [32000/60000 (53%)] Loss: 0.118246#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 43 [38400/60000 (64%)] Loss: 0.135111#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 43 [44800/60000 (75%)] Loss: 0.118639#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 43 [51200/60000 (85%)] Loss: 0.109465#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 43 [57600/60000 (96%)] Loss: 0.080287#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0343, Accuracy: 9891/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 44 [6400/60000 (11%)] Loss: 0.036705#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 44 [12800/60000 (21%)] Loss: 0.068862#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 44 [19200/60000 (32%)] Loss: 0.029734#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 44 [25600/60000 (43%)] Loss: 0.100908#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 44 [32000/60000 (53%)] Loss: 0.065856#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 44 [38400/60000 (64%)] Loss: 0.065901#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 44 [44800/60000 (75%)] Loss: 0.043819#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 44 [51200/60000 (85%)] Loss: 0.208306#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 44 [57600/60000 (96%)] Loss: 0.020173#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0351, Accuracy: 9887/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 45 [6400/60000 (11%)] Loss: 0.180203#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 45 [12800/60000 (21%)] Loss: 0.063422#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 45 [19200/60000 (32%)] Loss: 0.082477#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 45 [25600/60000 (43%)] Loss: 0.095993#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 45 [32000/60000 (53%)] Loss: 0.096017#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 45 [38400/60000 (64%)] Loss: 0.025780#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 45 [44800/60000 (75%)] Loss: 0.126184#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 45 [51200/60000 (85%)] Loss: 0.025497#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 45 [57600/60000 (96%)] Loss: 0.066355#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0365, Accuracy: 9887/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 46 [6400/60000 (11%)] Loss: 0.038022#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 46 [12800/60000 (21%)] Loss: 0.253038#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 46 [19200/60000 (32%)] Loss: 0.041427#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 46 [25600/60000 (43%)] Loss: 0.200566#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 46 [32000/60000 (53%)] Loss: 0.246226#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 46 [38400/60000 (64%)] Loss: 0.242621#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 46 [44800/60000 (75%)] Loss: 0.091123#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 46 [51200/60000 (85%)] Loss: 0.139429#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 46 [57600/60000 (96%)] Loss: 0.048305#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0345, Accuracy: 9895/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 47 [6400/60000 (11%)] Loss: 0.087813#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 47 [12800/60000 (21%)] Loss: 0.175875#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 47 [19200/60000 (32%)] Loss: 0.114143#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 47 [25600/60000 (43%)] Loss: 0.117313#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 47 [32000/60000 (53%)] Loss: 0.095179#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 47 [38400/60000 (64%)] Loss: 0.271099#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 47 [44800/60000 (75%)] Loss: 0.148610#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 47 [51200/60000 (85%)] Loss: 0.048792#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 47 [57600/60000 (96%)] Loss: 0.106460#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0343, Accuracy: 9895/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 48 [6400/60000 (11%)] Loss: 0.159438#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 48 [12800/60000 (21%)] Loss: 0.102538#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 48 [19200/60000 (32%)] Loss: 0.043739#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 48 [25600/60000 (43%)] Loss: 0.056753#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 48 [32000/60000 (53%)] Loss: 0.196211#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 48 [38400/60000 (64%)] Loss: 0.155105#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 48 [44800/60000 (75%)] Loss: 0.190288#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 48 [51200/60000 (85%)] Loss: 0.298425#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 48 [57600/60000 (96%)] Loss: 0.120141#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0360, Accuracy: 9885/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 49 [6400/60000 (11%)] Loss: 0.100056#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 49 [12800/60000 (21%)] Loss: 0.038950#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 49 [19200/60000 (32%)] Loss: 0.152304#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 49 [25600/60000 (43%)] Loss: 0.254672#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 49 [32000/60000 (53%)] Loss: 0.101048#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 49 [38400/60000 (64%)] Loss: 0.068503#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 49 [44800/60000 (75%)] Loss: 0.206642#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 49 [51200/60000 (85%)] Loss: 0.050891#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 49 [57600/60000 (96%)] Loss: 0.174104#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0356, Accuracy: 9894/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 50 [6400/60000 (11%)] Loss: 0.204063#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 50 [12800/60000 (21%)] Loss: 0.173029#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 50 [19200/60000 (32%)] Loss: 0.109832#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 50 [25600/60000 (43%)] Loss: 0.227216#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 50 [32000/60000 (53%)] Loss: 0.106297#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 50 [38400/60000 (64%)] Loss: 0.005486#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 50 [44800/60000 (75%)] Loss: 0.042832#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 50 [51200/60000 (85%)] Loss: 0.074624#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 50 [57600/60000 (96%)] Loss: 0.038454#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0347, Accuracy: 9901/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 51 [6400/60000 (11%)] Loss: 0.019694#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 51 [12800/60000 (21%)] Loss: 0.018403#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 51 [19200/60000 (32%)] Loss: 0.034463#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 51 [25600/60000 (43%)] Loss: 0.020460#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 51 [32000/60000 (53%)] Loss: 0.200257#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 51 [38400/60000 (64%)] Loss: 0.060592#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 51 [44800/60000 (75%)] Loss: 0.058124#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 51 [51200/60000 (85%)] Loss: 0.077916#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 51 [57600/60000 (96%)] Loss: 0.059043#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0348, Accuracy: 9898/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 52 [6400/60000 (11%)] Loss: 0.065269#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 52 [12800/60000 (21%)] Loss: 0.379531#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 52 [19200/60000 (32%)] Loss: 0.080676#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 52 [25600/60000 (43%)] Loss: 0.135848#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 52 [32000/60000 (53%)] Loss: 0.068347#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 52 [38400/60000 (64%)] Loss: 0.005696#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 52 [44800/60000 (75%)] Loss: 0.123619#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 52 [51200/60000 (85%)] Loss: 0.127650#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 52 [57600/60000 (96%)] Loss: 0.116734#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0329, Accuracy: 9893/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 53 [6400/60000 (11%)] Loss: 0.128041#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 53 [12800/60000 (21%)] Loss: 0.085114#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 53 [19200/60000 (32%)] Loss: 0.265462#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 53 [25600/60000 (43%)] Loss: 0.055618#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 53 [32000/60000 (53%)] Loss: 0.214720#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 53 [38400/60000 (64%)] Loss: 0.094847#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 53 [44800/60000 (75%)] Loss: 0.291006#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 53 [51200/60000 (85%)] Loss: 0.129130#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 53 [57600/60000 (96%)] Loss: 0.037581#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0331, Accuracy: 9891/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 54 [6400/60000 (11%)] Loss: 0.087481#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 54 [12800/60000 (21%)] Loss: 0.179968#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 54 [19200/60000 (32%)] Loss: 0.150827#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 54 [25600/60000 (43%)] Loss: 0.111437#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 54 [32000/60000 (53%)] Loss: 0.160384#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 54 [38400/60000 (64%)] Loss: 0.070910#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 54 [44800/60000 (75%)] Loss: 0.135381#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 54 [51200/60000 (85%)] Loss: 0.142174#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 54 [57600/60000 (96%)] Loss: 0.019497#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0335, Accuracy: 9898/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 55 [6400/60000 (11%)] Loss: 0.081622#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 55 [12800/60000 (21%)] Loss: 0.045023#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 55 [19200/60000 (32%)] Loss: 0.161775#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 55 [25600/60000 (43%)] Loss: 0.102856#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 55 [32000/60000 (53%)] Loss: 0.036828#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 55 [38400/60000 (64%)] Loss: 0.344084#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 55 [44800/60000 (75%)] Loss: 0.129132#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 55 [51200/60000 (85%)] Loss: 0.204599#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 55 [57600/60000 (96%)] Loss: 0.190473#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0339, Accuracy: 9893/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 56 [6400/60000 (11%)] Loss: 0.052145#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 56 [12800/60000 (21%)] Loss: 0.134496#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 56 [19200/60000 (32%)] Loss: 0.179634#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 56 [25600/60000 (43%)] Loss: 0.034100#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 56 [32000/60000 (53%)] Loss: 0.050627#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 56 [38400/60000 (64%)] Loss: 0.077196#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 56 [44800/60000 (75%)] Loss: 0.157859#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 56 [51200/60000 (85%)] Loss: 0.087625#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 56 [57600/60000 (96%)] Loss: 0.299618#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0319, Accuracy: 9899/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 57 [6400/60000 (11%)] Loss: 0.026706#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 57 [12800/60000 (21%)] Loss: 0.090827#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 57 [19200/60000 (32%)] Loss: 0.061407#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 57 [25600/60000 (43%)] Loss: 0.023857#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 57 [32000/60000 (53%)] Loss: 0.034828#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 57 [38400/60000 (64%)] Loss: 0.147661#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 57 [44800/60000 (75%)] Loss: 0.067487#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 57 [51200/60000 (85%)] Loss: 0.040971#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 57 [57600/60000 (96%)] Loss: 0.050816#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0325, Accuracy: 9898/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 58 [6400/60000 (11%)] Loss: 0.130971#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 58 [12800/60000 (21%)] Loss: 0.106368#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 58 [19200/60000 (32%)] Loss: 0.121631#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 58 [25600/60000 (43%)] Loss: 0.229219#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 58 [32000/60000 (53%)] Loss: 0.187055#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 58 [38400/60000 (64%)] Loss: 0.064304#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 58 [44800/60000 (75%)] Loss: 0.018331#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 58 [51200/60000 (85%)] Loss: 0.010630#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 58 [57600/60000 (96%)] Loss: 0.169514#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0337, Accuracy: 9889/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 59 [6400/60000 (11%)] Loss: 0.103137#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 59 [12800/60000 (21%)] Loss: 0.058214#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 59 [19200/60000 (32%)] Loss: 0.275803#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 59 [25600/60000 (43%)] Loss: 0.119863#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 59 [32000/60000 (53%)] Loss: 0.216173#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 59 [38400/60000 (64%)] Loss: 0.065875#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 59 [44800/60000 (75%)] Loss: 0.079435#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 59 [51200/60000 (85%)] Loss: 0.037891#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 59 [57600/60000 (96%)] Loss: 0.117010#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0335, Accuracy: 9898/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 60 [6400/60000 (11%)] Loss: 0.031274#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 60 [12800/60000 (21%)] Loss: 0.067187#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 60 [19200/60000 (32%)] Loss: 0.242228#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 60 [25600/60000 (43%)] Loss: 0.179213#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 60 [32000/60000 (53%)] Loss: 0.174482#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 60 [38400/60000 (64%)] Loss: 0.070687#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 60 [44800/60000 (75%)] Loss: 0.187256#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 60 [51200/60000 (85%)] Loss: 0.112936#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 60 [57600/60000 (96%)] Loss: 0.240457#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0336, Accuracy: 9896/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 61 [6400/60000 (11%)] Loss: 0.165502#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 61 [12800/60000 (21%)] Loss: 0.050630#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 61 [19200/60000 (32%)] Loss: 0.051875#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 61 [25600/60000 (43%)] Loss: 0.076306#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 61 [32000/60000 (53%)] Loss: 0.019781#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 61 [38400/60000 (64%)] Loss: 0.231782#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 61 [44800/60000 (75%)] Loss: 0.024237#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 61 [51200/60000 (85%)] Loss: 0.107273#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 61 [57600/60000 (96%)] Loss: 0.159676#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0339, Accuracy: 9898/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 62 [6400/60000 (11%)] Loss: 0.074403#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 62 [12800/60000 (21%)] Loss: 0.058190#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 62 [19200/60000 (32%)] Loss: 0.139564#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 62 [25600/60000 (43%)] Loss: 0.155171#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 62 [32000/60000 (53%)] Loss: 0.145582#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 62 [38400/60000 (64%)] Loss: 0.092231#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 62 [44800/60000 (75%)] Loss: 0.112156#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 62 [51200/60000 (85%)] Loss: 0.200901#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 62 [57600/60000 (96%)] Loss: 0.143675#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0324, Accuracy: 9903/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 63 [6400/60000 (11%)] Loss: 0.245128#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 63 [12800/60000 (21%)] Loss: 0.120037#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 63 [19200/60000 (32%)] Loss: 0.048563#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 63 [25600/60000 (43%)] Loss: 0.097951#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 63 [32000/60000 (53%)] Loss: 0.030139#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 63 [38400/60000 (64%)] Loss: 0.074505#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 63 [44800/60000 (75%)] Loss: 0.093680#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 63 [51200/60000 (85%)] Loss: 0.154194#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 63 [57600/60000 (96%)] Loss: 0.049299#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0333, Accuracy: 9902/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 64 [6400/60000 (11%)] Loss: 0.107199#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 64 [12800/60000 (21%)] Loss: 0.248483#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 64 [19200/60000 (32%)] Loss: 0.136995#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 64 [25600/60000 (43%)] Loss: 0.029165#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 64 [32000/60000 (53%)] Loss: 0.134294#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 64 [38400/60000 (64%)] Loss: 0.068187#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 64 [44800/60000 (75%)] Loss: 0.113829#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 64 [51200/60000 (85%)] Loss: 0.074076#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 64 [57600/60000 (96%)] Loss: 0.065282#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0341, Accuracy: 9893/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 65 [6400/60000 (11%)] Loss: 0.185330#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 65 [12800/60000 (21%)] Loss: 0.077942#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 65 [19200/60000 (32%)] Loss: 0.088431#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 65 [25600/60000 (43%)] Loss: 0.048355#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 65 [32000/60000 (53%)] Loss: 0.087714#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 65 [38400/60000 (64%)] Loss: 0.095065#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 65 [44800/60000 (75%)] Loss: 0.027056#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 65 [51200/60000 (85%)] Loss: 0.057265#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 65 [57600/60000 (96%)] Loss: 0.054746#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0347, Accuracy: 9892/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 66 [6400/60000 (11%)] Loss: 0.059838#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 66 [12800/60000 (21%)] Loss: 0.060135#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 66 [19200/60000 (32%)] Loss: 0.038968#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 66 [25600/60000 (43%)] Loss: 0.029561#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 66 [32000/60000 (53%)] Loss: 0.076519#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 66 [38400/60000 (64%)] Loss: 0.062314#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 66 [44800/60000 (75%)] Loss: 0.087165#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 66 [51200/60000 (85%)] Loss: 0.077644#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 66 [57600/60000 (96%)] Loss: 0.012222#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0320, Accuracy: 9899/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 67 [6400/60000 (11%)] Loss: 0.049246#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 67 [12800/60000 (21%)] Loss: 0.043905#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 67 [19200/60000 (32%)] Loss: 0.069530#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 67 [25600/60000 (43%)] Loss: 0.123376#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 67 [32000/60000 (53%)] Loss: 0.126443#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 67 [38400/60000 (64%)] Loss: 0.044682#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 67 [44800/60000 (75%)] Loss: 0.050416#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 67 [51200/60000 (85%)] Loss: 0.127809#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 67 [57600/60000 (96%)] Loss: 0.076310#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0324, Accuracy: 9897/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 68 [6400/60000 (11%)] Loss: 0.033351#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 68 [12800/60000 (21%)] Loss: 0.203236#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 68 [19200/60000 (32%)] Loss: 0.102300#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 68 [25600/60000 (43%)] Loss: 0.058467#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 68 [32000/60000 (53%)] Loss: 0.174342#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 68 [38400/60000 (64%)] Loss: 0.109744#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 68 [44800/60000 (75%)] Loss: 0.069767#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 68 [51200/60000 (85%)] Loss: 0.063789#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 68 [57600/60000 (96%)] Loss: 0.036223#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0343, Accuracy: 9892/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 69 [6400/60000 (11%)] Loss: 0.123904#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 69 [12800/60000 (21%)] Loss: 0.039564#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 69 [19200/60000 (32%)] Loss: 0.082209#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 69 [25600/60000 (43%)] Loss: 0.010368#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 69 [32000/60000 (53%)] Loss: 0.080549#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 69 [38400/60000 (64%)] Loss: 0.093462#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 69 [44800/60000 (75%)] Loss: 0.124006#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 69 [51200/60000 (85%)] Loss: 0.054083#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 69 [57600/60000 (96%)] Loss: 0.195720#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0320, Accuracy: 9897/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 70 [6400/60000 (11%)] Loss: 0.091919#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 70 [12800/60000 (21%)] Loss: 0.111875#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 70 [19200/60000 (32%)] Loss: 0.063098#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 70 [25600/60000 (43%)] Loss: 0.054583#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 70 [32000/60000 (53%)] Loss: 0.051599#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 70 [38400/60000 (64%)] Loss: 0.148447#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 70 [44800/60000 (75%)] Loss: 0.016186#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 70 [51200/60000 (85%)] Loss: 0.061116#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 70 [57600/60000 (96%)] Loss: 0.155298#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0319, Accuracy: 9906/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 71 [6400/60000 (11%)] Loss: 0.131618#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 71 [12800/60000 (21%)] Loss: 0.033141#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 71 [19200/60000 (32%)] Loss: 0.100116#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 71 [25600/60000 (43%)] Loss: 0.045885#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 71 [32000/60000 (53%)] Loss: 0.087338#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 71 [38400/60000 (64%)] Loss: 0.169835#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 71 [44800/60000 (75%)] Loss: 0.119572#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 71 [51200/60000 (85%)] Loss: 0.044750#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 71 [57600/60000 (96%)] Loss: 0.240835#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0317, Accuracy: 9907/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 72 [6400/60000 (11%)] Loss: 0.271560#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 72 [12800/60000 (21%)] Loss: 0.028900#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 72 [19200/60000 (32%)] Loss: 0.210168#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 72 [25600/60000 (43%)] Loss: 0.109116#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 72 [32000/60000 (53%)] Loss: 0.141454#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 72 [38400/60000 (64%)] Loss: 0.168613#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 72 [44800/60000 (75%)] Loss: 0.219056#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 72 [51200/60000 (85%)] Loss: 0.060399#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 72 [57600/60000 (96%)] Loss: 0.043493#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0313, Accuracy: 9904/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 73 [6400/60000 (11%)] Loss: 0.142548#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 73 [12800/60000 (21%)] Loss: 0.281666#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 73 [19200/60000 (32%)] Loss: 0.029255#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 73 [25600/60000 (43%)] Loss: 0.055622#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 73 [32000/60000 (53%)] Loss: 0.250654#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 73 [38400/60000 (64%)] Loss: 0.102586#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 73 [44800/60000 (75%)] Loss: 0.077563#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 73 [51200/60000 (85%)] Loss: 0.046932#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 73 [57600/60000 (96%)] Loss: 0.151983#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0320, Accuracy: 9899/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 74 [6400/60000 (11%)] Loss: 0.125363#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 74 [12800/60000 (21%)] Loss: 0.039450#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 74 [19200/60000 (32%)] Loss: 0.054856#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 74 [25600/60000 (43%)] Loss: 0.099377#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 74 [32000/60000 (53%)] Loss: 0.019091#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 74 [38400/60000 (64%)] Loss: 0.047556#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 74 [44800/60000 (75%)] Loss: 0.093003#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 74 [51200/60000 (85%)] Loss: 0.100626#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 74 [57600/60000 (96%)] Loss: 0.162626#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0353, Accuracy: 9887/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 75 [6400/60000 (11%)] Loss: 0.080359#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 75 [12800/60000 (21%)] Loss: 0.063050#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 75 [19200/60000 (32%)] Loss: 0.103407#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 75 [25600/60000 (43%)] Loss: 0.014740#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 75 [32000/60000 (53%)] Loss: 0.151761#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 75 [38400/60000 (64%)] Loss: 0.087658#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 75 [44800/60000 (75%)] Loss: 0.047844#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 75 [51200/60000 (85%)] Loss: 0.030707#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 75 [57600/60000 (96%)] Loss: 0.046915#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0328, Accuracy: 9901/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 76 [6400/60000 (11%)] Loss: 0.047835#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 76 [12800/60000 (21%)] Loss: 0.144650#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 76 [19200/60000 (32%)] Loss: 0.040655#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 76 [25600/60000 (43%)] Loss: 0.035912#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 76 [32000/60000 (53%)] Loss: 0.066303#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 76 [38400/60000 (64%)] Loss: 0.081801#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 76 [44800/60000 (75%)] Loss: 0.173158#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 76 [51200/60000 (85%)] Loss: 0.134696#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 76 [57600/60000 (96%)] Loss: 0.161760#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0319, Accuracy: 9907/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 77 [6400/60000 (11%)] Loss: 0.074871#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 77 [12800/60000 (21%)] Loss: 0.234113#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 77 [19200/60000 (32%)] Loss: 0.105561#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 77 [25600/60000 (43%)] Loss: 0.066886#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 77 [32000/60000 (53%)] Loss: 0.045934#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 77 [38400/60000 (64%)] Loss: 0.033610#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 77 [44800/60000 (75%)] Loss: 0.037894#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 77 [51200/60000 (85%)] Loss: 0.065154#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 77 [57600/60000 (96%)] Loss: 0.032477#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0317, Accuracy: 9908/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 78 [6400/60000 (11%)] Loss: 0.196764#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 78 [12800/60000 (21%)] Loss: 0.038656#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 78 [19200/60000 (32%)] Loss: 0.036117#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 78 [25600/60000 (43%)] Loss: 0.034193#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 78 [32000/60000 (53%)] Loss: 0.045344#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 78 [38400/60000 (64%)] Loss: 0.084457#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 78 [44800/60000 (75%)] Loss: 0.042022#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 78 [51200/60000 (85%)] Loss: 0.306672#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 78 [57600/60000 (96%)] Loss: 0.030365#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0309, Accuracy: 9906/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 79 [6400/60000 (11%)] Loss: 0.052691#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 79 [12800/60000 (21%)] Loss: 0.129358#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 79 [19200/60000 (32%)] Loss: 0.082864#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 79 [25600/60000 (43%)] Loss: 0.041385#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 79 [32000/60000 (53%)] Loss: 0.061863#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 79 [38400/60000 (64%)] Loss: 0.050578#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 79 [44800/60000 (75%)] Loss: 0.122971#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 79 [51200/60000 (85%)] Loss: 0.079685#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 79 [57600/60000 (96%)] Loss: 0.066454#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0299, Accuracy: 9906/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 80 [6400/60000 (11%)] Loss: 0.046151#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 80 [12800/60000 (21%)] Loss: 0.055650#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 80 [19200/60000 (32%)] Loss: 0.159340#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 80 [25600/60000 (43%)] Loss: 0.143948#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 80 [32000/60000 (53%)] Loss: 0.045080#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 80 [38400/60000 (64%)] Loss: 0.019272#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 80 [44800/60000 (75%)] Loss: 0.077514#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 80 [51200/60000 (85%)] Loss: 0.042986#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 80 [57600/60000 (96%)] Loss: 0.091620#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0315, Accuracy: 9902/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 81 [6400/60000 (11%)] Loss: 0.056343#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 81 [12800/60000 (21%)] Loss: 0.037183#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 81 [19200/60000 (32%)] Loss: 0.087564#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 81 [25600/60000 (43%)] Loss: 0.077761#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 81 [32000/60000 (53%)] Loss: 0.067879#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 81 [38400/60000 (64%)] Loss: 0.108830#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 81 [44800/60000 (75%)] Loss: 0.057337#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 81 [51200/60000 (85%)] Loss: 0.079863#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 81 [57600/60000 (96%)] Loss: 0.021160#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0327, Accuracy: 9902/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 82 [6400/60000 (11%)] Loss: 0.038890#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 82 [12800/60000 (21%)] Loss: 0.092415#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 82 [19200/60000 (32%)] Loss: 0.072560#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 82 [25600/60000 (43%)] Loss: 0.030161#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 82 [32000/60000 (53%)] Loss: 0.124448#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 82 [38400/60000 (64%)] Loss: 0.069973#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 82 [44800/60000 (75%)] Loss: 0.179310#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 82 [51200/60000 (85%)] Loss: 0.062329#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 82 [57600/60000 (96%)] Loss: 0.077145#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0306, Accuracy: 9910/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 83 [6400/60000 (11%)] Loss: 0.050432#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 83 [12800/60000 (21%)] Loss: 0.148489#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 83 [19200/60000 (32%)] Loss: 0.111924#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 83 [25600/60000 (43%)] Loss: 0.188128#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 83 [32000/60000 (53%)] Loss: 0.026280#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 83 [38400/60000 (64%)] Loss: 0.012117#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 83 [44800/60000 (75%)] Loss: 0.042773#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 83 [51200/60000 (85%)] Loss: 0.130759#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 83 [57600/60000 (96%)] Loss: 0.120269#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0317, Accuracy: 9900/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 84 [6400/60000 (11%)] Loss: 0.130953#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 84 [12800/60000 (21%)] Loss: 0.068524#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 84 [19200/60000 (32%)] Loss: 0.062708#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 84 [25600/60000 (43%)] Loss: 0.124766#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 84 [32000/60000 (53%)] Loss: 0.120848#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 84 [38400/60000 (64%)] Loss: 0.185190#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 84 [44800/60000 (75%)] Loss: 0.059152#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 84 [51200/60000 (85%)] Loss: 0.041169#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 84 [57600/60000 (96%)] Loss: 0.197853#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0324, Accuracy: 9901/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 85 [6400/60000 (11%)] Loss: 0.169204#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 85 [12800/60000 (21%)] Loss: 0.025074#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 85 [19200/60000 (32%)] Loss: 0.076920#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 85 [25600/60000 (43%)] Loss: 0.022236#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 85 [32000/60000 (53%)] Loss: 0.051466#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 85 [38400/60000 (64%)] Loss: 0.030869#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 85 [44800/60000 (75%)] Loss: 0.168445#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 85 [51200/60000 (85%)] Loss: 0.050330#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 85 [57600/60000 (96%)] Loss: 0.040289#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0301, Accuracy: 9912/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 86 [6400/60000 (11%)] Loss: 0.064346#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 86 [12800/60000 (21%)] Loss: 0.161830#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 86 [19200/60000 (32%)] Loss: 0.250034#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 86 [25600/60000 (43%)] Loss: 0.196496#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 86 [32000/60000 (53%)] Loss: 0.087708#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 86 [38400/60000 (64%)] Loss: 0.087858#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 86 [44800/60000 (75%)] Loss: 0.020622#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 86 [51200/60000 (85%)] Loss: 0.048353#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 86 [57600/60000 (96%)] Loss: 0.111608#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0334, Accuracy: 9899/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 87 [6400/60000 (11%)] Loss: 0.038829#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 87 [12800/60000 (21%)] Loss: 0.092634#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 87 [19200/60000 (32%)] Loss: 0.031339#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 87 [25600/60000 (43%)] Loss: 0.045145#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 87 [32000/60000 (53%)] Loss: 0.033340#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 87 [38400/60000 (64%)] Loss: 0.015641#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 87 [44800/60000 (75%)] Loss: 0.121507#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 87 [51200/60000 (85%)] Loss: 0.049141#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 87 [57600/60000 (96%)] Loss: 0.008309#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0304, Accuracy: 9907/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 88 [6400/60000 (11%)] Loss: 0.043641#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 88 [12800/60000 (21%)] Loss: 0.259867#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 88 [19200/60000 (32%)] Loss: 0.249357#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 88 [25600/60000 (43%)] Loss: 0.017447#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 88 [32000/60000 (53%)] Loss: 0.025209#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 88 [38400/60000 (64%)] Loss: 0.126096#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 88 [44800/60000 (75%)] Loss: 0.122665#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 88 [51200/60000 (85%)] Loss: 0.116750#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 88 [57600/60000 (96%)] Loss: 0.143116#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0317, Accuracy: 9899/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 89 [6400/60000 (11%)] Loss: 0.076101#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 89 [12800/60000 (21%)] Loss: 0.093926#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 89 [19200/60000 (32%)] Loss: 0.046323#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 89 [25600/60000 (43%)] Loss: 0.160298#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 89 [32000/60000 (53%)] Loss: 0.059707#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 89 [38400/60000 (64%)] Loss: 0.108461#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 89 [44800/60000 (75%)] Loss: 0.104760#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 89 [51200/60000 (85%)] Loss: 0.122855#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 89 [57600/60000 (96%)] Loss: 0.184643#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0331, Accuracy: 9905/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 90 [6400/60000 (11%)] Loss: 0.099659#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 90 [12800/60000 (21%)] Loss: 0.048984#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 90 [19200/60000 (32%)] Loss: 0.043429#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 90 [25600/60000 (43%)] Loss: 0.049406#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 90 [32000/60000 (53%)] Loss: 0.005740#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 90 [38400/60000 (64%)] Loss: 0.074369#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 90 [44800/60000 (75%)] Loss: 0.018109#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 90 [51200/60000 (85%)] Loss: 0.079219#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 90 [57600/60000 (96%)] Loss: 0.062054#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0328, Accuracy: 9893/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 91 [6400/60000 (11%)] Loss: 0.195896#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 91 [12800/60000 (21%)] Loss: 0.114285#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 91 [19200/60000 (32%)] Loss: 0.157614#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 91 [25600/60000 (43%)] Loss: 0.068713#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 91 [32000/60000 (53%)] Loss: 0.031988#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 91 [38400/60000 (64%)] Loss: 0.276832#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 91 [44800/60000 (75%)] Loss: 0.019260#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 91 [51200/60000 (85%)] Loss: 0.115912#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 91 [57600/60000 (96%)] Loss: 0.084306#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0319, Accuracy: 9907/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 92 [6400/60000 (11%)] Loss: 0.072547#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 92 [12800/60000 (21%)] Loss: 0.045770#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 92 [19200/60000 (32%)] Loss: 0.110325#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 92 [25600/60000 (43%)] Loss: 0.117409#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 92 [32000/60000 (53%)] Loss: 0.043965#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 92 [38400/60000 (64%)] Loss: 0.061689#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 92 [44800/60000 (75%)] Loss: 0.085295#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 92 [51200/60000 (85%)] Loss: 0.047310#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 92 [57600/60000 (96%)] Loss: 0.076075#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0320, Accuracy: 9904/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 93 [6400/60000 (11%)] Loss: 0.058955#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 93 [12800/60000 (21%)] Loss: 0.002578#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 93 [19200/60000 (32%)] Loss: 0.117825#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 93 [25600/60000 (43%)] Loss: 0.124605#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 93 [32000/60000 (53%)] Loss: 0.087118#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 93 [38400/60000 (64%)] Loss: 0.122601#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 93 [44800/60000 (75%)] Loss: 0.074069#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 93 [51200/60000 (85%)] Loss: 0.075108#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 93 [57600/60000 (96%)] Loss: 0.072413#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0329, Accuracy: 9905/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 94 [6400/60000 (11%)] Loss: 0.030512#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 94 [12800/60000 (21%)] Loss: 0.068925#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 94 [19200/60000 (32%)] Loss: 0.137267#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 94 [25600/60000 (43%)] Loss: 0.042190#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 94 [32000/60000 (53%)] Loss: 0.060325#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 94 [38400/60000 (64%)] Loss: 0.041429#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 94 [44800/60000 (75%)] Loss: 0.107717#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 94 [51200/60000 (85%)] Loss: 0.102434#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 94 [57600/60000 (96%)] Loss: 0.019144#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0316, Accuracy: 9904/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 95 [6400/60000 (11%)] Loss: 0.056783#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 95 [12800/60000 (21%)] Loss: 0.080118#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 95 [19200/60000 (32%)] Loss: 0.032920#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 95 [25600/60000 (43%)] Loss: 0.026927#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 95 [32000/60000 (53%)] Loss: 0.149326#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 95 [38400/60000 (64%)] Loss: 0.049186#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 95 [44800/60000 (75%)] Loss: 0.132033#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 95 [51200/60000 (85%)] Loss: 0.038827#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 95 [57600/60000 (96%)] Loss: 0.062478#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0332, Accuracy: 9896/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 96 [6400/60000 (11%)] Loss: 0.034881#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 96 [12800/60000 (21%)] Loss: 0.092150#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 96 [19200/60000 (32%)] Loss: 0.096903#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 96 [25600/60000 (43%)] Loss: 0.266187#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 96 [32000/60000 (53%)] Loss: 0.054809#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 96 [38400/60000 (64%)] Loss: 0.061763#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 96 [44800/60000 (75%)] Loss: 0.072388#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 96 [51200/60000 (85%)] Loss: 0.081332#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 96 [57600/60000 (96%)] Loss: 0.053196#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0325, Accuracy: 9897/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 97 [6400/60000 (11%)] Loss: 0.028507#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 97 [12800/60000 (21%)] Loss: 0.059722#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 97 [19200/60000 (32%)] Loss: 0.160479#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 97 [25600/60000 (43%)] Loss: 0.147984#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 97 [32000/60000 (53%)] Loss: 0.091091#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 97 [38400/60000 (64%)] Loss: 0.105767#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 97 [44800/60000 (75%)] Loss: 0.112072#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 97 [51200/60000 (85%)] Loss: 0.022701#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 97 [57600/60000 (96%)] Loss: 0.265562#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0316, Accuracy: 9903/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 98 [6400/60000 (11%)] Loss: 0.032463#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 98 [12800/60000 (21%)] Loss: 0.038528#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 98 [19200/60000 (32%)] Loss: 0.087709#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 98 [25600/60000 (43%)] Loss: 0.073290#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 98 [32000/60000 (53%)] Loss: 0.154298#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 98 [38400/60000 (64%)] Loss: 0.096590#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 98 [44800/60000 (75%)] Loss: 0.065880#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 98 [51200/60000 (85%)] Loss: 0.103809#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 98 [57600/60000 (96%)] Loss: 0.058233#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0316, Accuracy: 9912/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 99 [6400/60000 (11%)] Loss: 0.053864#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 99 [12800/60000 (21%)] Loss: 0.079291#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 99 [19200/60000 (32%)] Loss: 0.136426#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 99 [25600/60000 (43%)] Loss: 0.033091#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 99 [32000/60000 (53%)] Loss: 0.041876#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 99 [38400/60000 (64%)] Loss: 0.055562#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 99 [44800/60000 (75%)] Loss: 0.066481#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 99 [51200/60000 (85%)] Loss: 0.058545#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 99 [57600/60000 (96%)] Loss: 0.173835#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0323, Accuracy: 9896/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 100 [6400/60000 (11%)] Loss: 0.110165#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 100 [12800/60000 (21%)] Loss: 0.197341#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 100 [19200/60000 (32%)] Loss: 0.052914#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 100 [25600/60000 (43%)] Loss: 0.028043#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 100 [32000/60000 (53%)] Loss: 0.098189#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 100 [38400/60000 (64%)] Loss: 0.136447#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 100 [44800/60000 (75%)] Loss: 0.123920#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 100 [51200/60000 (85%)] Loss: 0.095477#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 100 [57600/60000 (96%)] Loss: 0.034057#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.0310, Accuracy: 9911/10000 (99%)#015\u001b[0m\n",
      "\u001b[34m#015\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.#015\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mwandb#033[0m: #033[33mWARNING#033[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")#015\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: - 2.558 MB of 2.561 MB uploaded (0.026 MB deduped)#015wandb: \\ 2.647 MB of 2.647 MB uploaded (0.026 MB deduped)#015wandb: | 2.647 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: / 2.647 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: - 2.647 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: \\ 2.765 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: | 2.765 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: / 2.780 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: - 2.780 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: \\ 2.780 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: | 2.780 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: / 2.780 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: - 2.780 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb: \\ 2.780 MB of 2.780 MB uploaded (0.026 MB deduped)#015wandb:                                                                                \u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:  testing/loss █▄▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u001b[0m\n",
      "\u001b[34mwandb: training/loss █▄▄▂▄▃▃▂▁▄▂▃▂▁▁▂▂▂▃▁▄▃▂▁▂▁▂▃▅▅▂▂▃▂▃▄▄▅▂▁\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:  testing/loss 0.03098\u001b[0m\n",
      "\u001b[34mwandb: training/loss 0.02259\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Synced pytorch-training-2022-05-31-05-11-19-197-algo-1: https://wandb.ai/yito/sm-pytorch-mnist-studio/runs/pytorch-training-2022-05-31-05-11-19-197-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 5 W&B file(s), 100 media file(s), 200 artifact file(s) and 1 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20220531_051400-pytorch-training-2022-05-31-05-11-19-197-algo-1/logs\u001b[0m\n",
      "\n",
      "2022-05-31 05:41:34 Completed - Training job completed\n",
      "ProfilerReport-1653973879: NoIssuesFound\n",
      "Training seconds: 1716\n",
      "Billable seconds: 1716\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs},\n",
    "    # trialの情報を指定\n",
    "    experiment_config={\n",
    "        \"ExperimentName\": experiment.experiment_name,\n",
    "        \"TrialName\": experiments_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\": estimator_trial_component.display_name,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複数ジョブ発行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlrs = [0.01, 0.001, 0.05, 0.1, 0.2]\\nfor lr in lrs:\\n    estimator = PyTorch(entry_point=\\'mnist.py\\',\\n                        source_dir=\"src\",\\n                        role=role,\\n                        py_version=\\'py3\\',\\n                        framework_version=\\'1.8.0\\',\\n                        instance_count=1,\\n                        instance_type=\\'ml.c5.2xlarge\\',\\n                        hyperparameters={\\n                            \\'epochs\\': 10,\\n                            \\'backend\\': \\'gloo\\',\\n                            \\'lr\\': lr,\\n                        },\\n                       environment={\"WANDB_API_KEY\": current_api_key})\\n    estimator.fit({\\'training\\': inputs}, wait=False)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "lrs = [0.01, 0.001, 0.05, 0.1, 0.2]\n",
    "for lr in lrs:\n",
    "    estimator = PyTorch(entry_point='mnist.py',\n",
    "                        source_dir=\"src\",\n",
    "                        role=role,\n",
    "                        py_version='py3',\n",
    "                        framework_version='1.8.0',\n",
    "                        instance_count=1,\n",
    "                        instance_type='ml.c5.2xlarge',\n",
    "                        hyperparameters={\n",
    "                            'epochs': 10,\n",
    "                            'backend': 'gloo',\n",
    "                            'lr': lr,\n",
    "                        },\n",
    "                       environment={\"WANDB_API_KEY\": current_api_key})\n",
    "    estimator.fit({'training': inputs}, wait=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2022-05-31-04-08-46-144\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2022-05-31-04-08-47-368\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2022-05-31-04-08-50-231\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2022-05-31-04-08-51-223\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2022-05-31-04-08-52-342\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.01, 0.001, 0.05, 0.1, 0.2]\n",
    "for lr in lrs:\n",
    "    estimator = PyTorch(entry_point='mnist.py',\n",
    "                        source_dir=\"src\",\n",
    "                        role=role,\n",
    "                        py_version='py3',\n",
    "                        framework_version='1.8.0',\n",
    "                        instance_count=1,\n",
    "                        instance_type='ml.c5.2xlarge',\n",
    "                        hyperparameters={\n",
    "                            'epochs': 100,\n",
    "                            'backend': 'gloo',\n",
    "                            'lr': lr,\n",
    "                        },\n",
    "                        metric_definitions=[\n",
    "                            {'Name': 'training/loss', 'Regex': 'Loss: (.*?)#015'},\n",
    "                            #{'Name': 'Validation Loss', 'Regex': 'val_loss: (.*?);'},\n",
    "                            {'Name': 'test/loss', 'Regex': 'Average loss: (.*?),'},\n",
    "                        ],\n",
    "                        environment={\"WANDB_API_KEY\": current_api_key})\n",
    "    \n",
    "    estimator.fit({'training': inputs},\n",
    "        # trialの情報を指定\n",
    "        experiment_config={\n",
    "            \"ExperimentName\": experiment.experiment_name,\n",
    "            \"TrialName\": experiments_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": estimator_trial_component.display_name,\n",
    "        },\n",
    "        wait=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/pytorch-1.10-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
